<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>KafKa基础原理</title>
      <link href="/KafKa/KafKa%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/"/>
      <url>/KafKa/KafKa%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h3 id="一、概念理解"><a href="#一、概念理解" class="headerlink" title="一、概念理解"></a>一、概念理解</h3><p><strong>Kafka</strong> 是最初由 Linkedin 公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于 zookeeper 协调的<strong>分布式消息系统</strong>，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于 hadoop 的批处理系统、低延迟的实时系统、storm&#x2F;Spark 流式处理引擎，web&#x2F;nginx 日志、访问日志，消息服务等等，用 scala 语言编写，Linkedin 于 2010 年贡献给了 Apache 基金会并成为顶级开源 项目。</p><h4 id="（1）产生背景"><a href="#（1）产生背景" class="headerlink" title="（1）产生背景"></a>（1）产生背景</h4><p>当今社会各种应用系统诸如商业、社交、搜索、浏览等像信息工厂一样不断的生产出各种信息，在大数据时代，我们面临如下几个挑战：</p><ol><li>如何收集这些巨大的信息</li><li>如何分析它</li><li>如何及时做到如上两点</li></ol><p>以上几个挑战形成了一个业务需求模型，即生产者生产（produce）各种信息，消费者消费（consume）（处理分析）这些信息，而在生产者与消费者之间，需要一个沟通两者的桥梁 - 消息系统。从一个微观层面来说，这种需求也可理解为不同的系统之间如何传递消息。</p><p><strong>Kafka 诞生</strong> Kafka 由 linked-in 开源 kafka - 即是解决上述这类问题的一个框架，它实现了生产者和消费者之间的无缝连接。 kafka - 高产出的分布式消息系统 (A high-throughput distributed messaging system)</p><h4 id="（2）Kafka-的特性"><a href="#（2）Kafka-的特性" class="headerlink" title="（2）Kafka 的特性"></a>（2）Kafka 的特性</h4><table><thead><tr><th>特性</th><th>分布式</th><th>高性能</th><th>持久性和扩展行</th></tr></thead><tbody><tr><td></td><td>多分区</td><td>高吞吐量</td><td>数据可持久化</td></tr><tr><td></td><td>多副本</td><td>低延迟</td><td>容错性</td></tr><tr><td></td><td>多订阅者</td><td>高并发</td><td>支持水平在线扩展</td></tr><tr><td></td><td>基于ZooKeeper调度</td><td>时间复杂度O（1）</td><td>消息自动平衡</td></tr></tbody></table><ul><li><strong>高吞吐量、低延迟</strong>：kafka 每秒可以处理几十万条消息，它的延迟最低只有几毫秒</li><li><strong>可扩展性</strong>：kafka 集群支持热扩展</li><li><strong>持久性、可靠性</strong>：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失</li><li><strong>容错性</strong>：允许集群中节点失败（若副本数量为 n, 则允许 n-1 个节点失败）</li><li><strong>高并发</strong>：支持数千个客户端同时读写</li></ul><h4 id="（3）Kafka-场景应用"><a href="#（3）Kafka-场景应用" class="headerlink" title="（3）Kafka 场景应用"></a>（3）Kafka 场景应用</h4><ul><li><strong>日志收集</strong>：一个公司可以用 Kafka 可以收集各种服务的 log，通过 kafka 以统一接口服务的方式开放给各种 consumer，例如 hadoop、Hbase、Solr 等。</li><li><strong>消息系统</strong>：解耦和生产者和消费者、缓存消息等。</li><li><strong>用户活动跟踪</strong>：Kafka 经常被用来记录 web 用户或者 app 用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到 kafka 的 topic 中，然后订阅者通过订阅这些 topic 来做实时的监控分析，或者装载到 hadoop、数据仓库中做离线分析和挖掘。</li><li><strong>运营指标</strong>：Kafka 也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。</li><li><strong>流式处理</strong>：比如 spark streaming 和 storm</li><li><strong>事件源</strong></li></ul><h4 id="（4）Kafka-一些重要设计思想"><a href="#（4）Kafka-一些重要设计思想" class="headerlink" title="（4）Kafka 一些重要设计思想"></a>（4）Kafka 一些重要设计思想</h4><p><img src="https://cdn.jsdelivr.net/gh/cgshao02/BLOG_images/img/image-20240924190102682.png" alt="KafKa架构"></p><ul><li><strong>Consumergroup</strong>：KafKa是按消费组来消费消息的，一个消费组下面的所有机器可以组成一个Consumer Group，每条消息只能被该Consumer Group一个Consumer消费，不容的Consumer Group可以消费同一条消息</li><li><strong>消息状态</strong>：在 Kafka 中，消息的状态被保存在 consumer 中，broker 不会关心哪个消息被消费了被谁消费了，只记录一个 offset 值（指向 partition 中下一个要被消费的消息位置），这就意味着如果 consumer 处理不好的话，broker 上的一个消息可能会被消费多次。</li><li><strong>消息持久化</strong>：Kafka 中会把消息持久化到本地文件系统中，并且保持极高的效率。</li><li><strong>消息有效期</strong>：Kafka 会长久保留其中的消息，以便 consumer 可以多次消费，当然其中很多细节是可配置的。</li><li><strong>批量发送</strong>：Kafka 支持以消息集合为单位进行批量发送，以提高 push 效率。</li><li><strong>push-and-pull</strong> :Kafka 中的 Producer 和 consumer 采用的是 push-and-pull 模式，即 Producer 只管向 broker push 消息，consumer 只管从 broker pull 消息，两者对消息的生产和消费是异步的。</li><li><strong>Kafka 集群中 broker 之间的关系</strong>：不是主从关系，各个 broker 在集群中地位一样，我们可以随意的增加或删除任何一个 broker 节点。</li><li><strong>负载均衡方面</strong>： Kafka 提供了一个 metadata API 来管理 broker 之间的负载（对 Kafka0.8.x 而言，对于 0.7.x 主要靠 zookeeper 来实现负载均衡）。</li><li><strong>同步异步</strong>：Producer 采用异步 push 方式，极大提高 Kafka 系统的吞吐率（可以通过参数控制是采用同步还是异步方式）。</li><li><strong>分区机制 partition</strong>：Kafka 的 broker 端支持消息分区，Producer 可以决定把消息发到哪个分区，在一个分区中消息的顺序就是 Producer 发送消息的顺序，一个主题中可以有多个分区，具体分区的数量是可配置的。分区的意义很重大，后面的内容会逐渐体现。</li><li><strong>离线数据装载</strong>：Kafka 由于对可拓展的数据持久化的支持，它也非常适合向 Hadoop 或者数据仓库中进行数据装载。</li><li><strong>插件支持</strong>：现在不少活跃的社区已经开发出不少插件来拓展 Kafka 的功能，如用来配合 Storm、Hadoop、flume 相关的插件。</li></ul><h3 id="二、消息队列通信的模式"><a href="#二、消息队列通信的模式" class="headerlink" title="二、消息队列通信的模式"></a>二、消息队列通信的模式</h3><h4 id="（1）点对点模式"><a href="#（1）点对点模式" class="headerlink" title="（1）点对点模式"></a>（1）点对点模式</h4><p><img src="https://cdn.jsdelivr.net/gh/cgshao02/BLOG_images/img/c076a56492c6ab4c1768ac8be584ad81.png"></p><p>如上图所示，点对点模式通常是基于拉取或者轮询的消息传送模型，这个模型的特点是发送到队列的消息被一个且只有一个消费者进行处理。生产者将消息放入消息队列后，由消费者主动的去拉取消息进行消费。点对点模型的的优点是消费者拉取消息的频率可以由自己控制。但是消息队列是否有消息需要消费，在消费者端无法感知，所以在消费者端需要额外的线程去监控。</p><h4 id="（2）发布订阅模式"><a href="#（2）发布订阅模式" class="headerlink" title="（2）发布订阅模式"></a>（2）发布订阅模式</h4><p><img src="https://ask.qcloudimg.com/http-save/yehe-8223537/45ac0cf0a61afe236bd86a8348ad9848.png"></p><p>如上图所示，发布订阅模式是一个基于消息送的消息传送模型，改模型可以有多种不同的订阅者。生产者将消息放入消息队列后，队列会将消息推送给订阅过该类消息的消费者（类似微信公众号）。由于是消费者被动接收推送，所以无需感知消息队列是否有待消费的消息！但是 consumer1、consumer2、consumer3 由于机器性能不一样，所以处理消息的能力也会不一样，但消息队列却无法感知消费者消费的速度！所以推送的速度成了发布订阅模模式的一个问题！假设三个消费者处理速度分别是 8M&#x2F;s、5M&#x2F;s、2M&#x2F;s，如果队列推送的速度为 5M&#x2F;s，则 consumer3 无法承受！如果队列推送的速度为 2M&#x2F;s，则 consumer1、consumer2 会出现资源的极大浪费！</p><h3 id="三、Kafka-的架构原理"><a href="#三、Kafka-的架构原理" class="headerlink" title="三、Kafka 的架构原理"></a>三、Kafka 的架构原理</h3><h4 id="（1）基础架构与名词解释"><a href="#（1）基础架构与名词解释" class="headerlink" title="（1）基础架构与名词解释"></a>（1）基础架构与名词解释</h4><p><img src="https://cdn.jsdelivr.net/gh/cgshao02/BLOG_images/img/935e34114a00ca407c376a24b340341b.png"></p><p>在这里插入图片描述</p><ul><li>Producer：Producer 即生产者，消息的产生者，是消息的入口。</li><li>Broker： 一个Kafka集群中的一台服务器就是一个Broker，Broker可以水平无限扩展，同一个Topic中的消息可以分布在多个Broker中。</li><li>Topic：消息的主题，可以理解为消息的分类，kafka 的数据就保存在 topic。在每个 broker 上都可以创建多个 topic。</li><li>Partition：Topic 的分区，每个 topic 可以有多个分区，分区的作用是做负载，提高 kafka 的吞吐量。同一个 topic 在不同的分区的数据是不重复的，partition 的表现形式就是一个一个的文件夹！</li><li>Replication: 每一个分区都有多个副本，副本的作用是做备胎。当主分区（Leader）故障的时候会选择一个备胎（Follower）上位，成为 Leader。在 kafka 中默认副本的最大数量是 10 个，且副本的数量不能大于 Broker 的数量，follower 和 leader 绝对是在不同的机器，同一机器对同一个分区也只可能存放一个副本（包括自己）。</li><li>Message：每一条发送的消息主体。</li><li>Consumer：消费者，即消息的消费方，是消息的出口。</li><li>Consumer Group：我们可以将多个消费组组成一个消费者组，在 kafka 的设计中同一个分区的数据只能被消费者组中的某一个消费者消费。同一个消费者组的消费者可以消费同一个 topic 的不同分区的数据，这也是为了提高 kafka 的吞吐量！</li><li>Zookeeper：kafka 集群依赖 zookeeper 来保存集群的的元信息，来保证系统的可用性。</li></ul><h4 id="（2）工作流程分析"><a href="#（2）工作流程分析" class="headerlink" title="（2）工作流程分析"></a>（2）工作流程分析</h4><h5 id="（1）发送数据"><a href="#（1）发送数据" class="headerlink" title="（1）发送数据"></a>（1）发送数据</h5><p>我们看上面的架构图中，producer 就是生产者，是数据的入口。注意看图中的红色箭头，<strong>Producer 在写入数据的时候永远的找 leader</strong>，不会直接将数据写入 <strong>follower</strong>！那 leader 怎么找呢？写入的流程又是什么样的呢？我们看下图：</p><p><img src="https://cdn.jsdelivr.net/gh/cgshao02/BLOG_images/img/882ff36ad77ac28c3ce824f299bcef26.png"></p><p>发送的流程就在图中已经说明了，就不单独在文字列出来了！需要注意的一点是，消息写入 leader 后，follower 是主动的去 leader 进行同步的！producer 采用 push 模式将数据发布到 broker，每条消息追加到分区中，顺序写入磁盘，所以保证同一分区内的数据是有序的！写入示意图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/cgshao02/BLOG_images/img/cb151f6c9a4bab2e5f42376b4514e335.png"></p><p>在这里插入图片描述上面说到数据会写入到不同的分区，那 kafka 为什么要做分区呢？相信大家应该也能猜到，分区的主要目的是：</p><ol><li><strong>方便扩展</strong>：因为一个 topic 可以有多个 partition，所以我们可以通过扩展机器去轻松的应对日益增长的数据量。</li><li><strong>提高并发</strong>：以 partition 为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率。</li></ol><p>熟悉负载均衡的朋友应该知道，当我们向某个服务器发送请求的时候，服务端可能会对请求做一个负载，将流量分发到不同的服务器，那在 kafka 中，如果某个 topic 有多个 partition，producer 又怎么知道该将数据发往哪个 partition 呢？</p><p>kafka 中有几个原则：</p><ol><li>partition 在写入的时候可以指定需要写入的 partition，如果有指定，则写入对应的 partition。</li><li>如果没有指定 partition，但是设置了数据的 key，则会根据 key 的值 hash 出一个 partition。</li><li>如果既没指定 partition，又没有设置 key，则会轮询选出一个 partition。</li></ol><p>保证消息不丢失是一个消息队列中间件的基本保证，那 producer 在向 kafka 写入消息的时候，怎么保证消息不丢失呢？其实上面的写入流程图中有描述出来，那就是通过 ACK 应答机制！在生产者向队列写入数据的时候可以设置参数来确定是否确认 kafka 接收到数据，这个参数可设置的值为 <strong>0、1、all</strong>。</p><ul><li>0 代表 producer 往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。</li><li>1 代表 producer 往集群发送数据只要 leader 应答就可以发送下一条，只确保 leader 发送成功。</li><li>all 代表 producer 往集群发送数据需要所有的 follower 都完成从 leader 的同步才会发送下一条，确保 leader 发送成功和所有的副本都完成备份。安全性最高，但是效率最低。</li></ul><p>最后要注意的是，如果往不存在的 topic 写数据，能不能写入成功呢？kafka 会自动创建 topic，分区和副本的数量根据默认配置都是 1。</p><h5 id="（2）保存数据"><a href="#（2）保存数据" class="headerlink" title="（2）保存数据"></a>（2）保存数据</h5><p>Producer 将数据写入 kafka 后，集群就需要对数据进行保存了！kafka 将数据保存在磁盘，可能在我们的一般的认知里，写入磁盘是比较耗时的操作，不适合这种高并发的组件。Kafka 初始会单独开辟一块磁盘空间，顺序写入数据（效率比随机写入高）。</p><h6 id="（1）Partition-结构"><a href="#（1）Partition-结构" class="headerlink" title="（1）Partition 结构"></a>（1）Partition 结构</h6><p>前面说过了每个 topic 都可以分为一个或多个 partition，如果你觉得 topic 比较抽象，那 partition 就是比较具体的东西了！Partition 在服务器上的表现形式就是一个一个的文件夹，每个 partition 的文件夹下面会有多组 segment 文件，每组 segment 文件又包含. index 文件、.log 文件、.timeindex 文件（早期版本中没有）三个文件， log 文件就实际是存储 message 的地方，而 index 和 timeindex 文件为索引文件，用于检索消息。</p><p><img src="https://cdn.jsdelivr.net/gh/cgshao02/BLOG_images/img/706d31956c65fa740a4bf906fc81cd5c.png"></p><p>如上图，这个 partition 有三组 segment 文件，每个 log 文件的大小是一样的，但是存储的 message 数量是不一定相等的（每条的 message 大小不一致）。文件的命名是以该 segment 最小 offset 来命名的，如 000.index 存储 offset 为 0~368795 的消息，kafka 就是利用分段 + 索引的方式来解决查找效率的问题。</p><h6 id="（2）Message-结构"><a href="#（2）Message-结构" class="headerlink" title="（2）Message 结构"></a>（2）Message 结构</h6><p>上面说到 log 文件就实际是存储 message 的地方，我们在 producer 往 kafka 写入的也是一条一条的 message，那存储在 log 中的 message 是什么样子的呢？消息主要包含消息体、消息大小、offset、压缩类型…… 等等！我们重点需要知道的是下面三个：</p><ul><li><strong>offset</strong>：offset 是一个占 8byte 的有序 id 号，它可以唯一确定每条消息在 parition 内的位置！</li><li><strong>消息大小</strong>：消息大小占用 4byte，用于描述消息的大小。</li><li><strong>消息体</strong>：消息体存放的是实际的消息数据（被压缩过），占用的空间根据具体的消息而不一样。</li></ul><h6 id="（3）存储策略"><a href="#（3）存储策略" class="headerlink" title="（3）存储策略"></a>（3）存储策略</h6><p>无论消息是否被消费，kafka 都会保存所有的消息。那对于旧数据有什么删除策略呢？</p><ul><li>基于时间，默认配置是 168 小时（7 天）。</li><li>基于大小，默认配置是 1073741824。</li></ul><p>需要注意的是，kafka 读取特定消息的时间复杂度是 O(1)，所以这里删除过期的文件并不会提高 kafka 的性能！</p><h5 id="（3）消费数据"><a href="#（3）消费数据" class="headerlink" title="（3）消费数据"></a>（3）消费数据</h5><p>消息存储在 log 文件后，消费者就可以进行消费了。在讲消息队列通信的两种模式的时候讲到过点对点模式和发布订阅模式。Kafka 采用的是发布订阅模式，消费者主动的去 kafka 集群拉取消息，与 producer 相同的是，消费者在拉取消息的时候也是找 <strong>leader</strong> 去拉取。</p><p><img src="https://cdn.jsdelivr.net/gh/cgshao02/BLOG_images/img/481a974a70b23c51494c8209b15b78f5.png"></p><p>图示是消费者组内的消费者小于 partition 数量的情况，所以会出现某个消费者消费多个 partition 数据的情况，消费的速度也就不及只处理一个 partition 的消费者的处理速度！如果是消费者组的消费者多于 partition 的数量，那会不会出现多个消费者消费同一个 partition 的数据呢？上面已经提到过不会出现这种情况！多出来的消费者不消费任何 partition 的数据。所以在实际的应用中，</p><p><u><strong>建议消费者组的 consumer 的数量与 partition 的数量一致</strong>！</u></p><p>在保存数据的小节里面，我们聊到了 partition 划分为多组 segment，每个 segment 又包含. log、.index、.timeindex 文件，存放的每条 message 包含 offset、消息大小、消息体…… 我们多次提到 segment 和 offset，查找消息的时候是怎么利用 segment+offset 配合查找的呢？假如现在需要查找一个 offset 为 368801 的 message 是什么样的过程呢？我们先看看下面的图：</p><p><img src="https://cdn.jsdelivr.net/gh/cgshao02/BLOG_images/img/9538b991029028098618a677fc88e0f0.png"></p><ol><li>先找到 offset 的 368801message 所在的 segment 文件（利用二分法查找），这里找到的就是在第二个 segment 文件。</li><li>打开找到的 segment 中的. index 文件（也就是 368796.index 文件，该文件起始偏移量为 368796+1，我们要查找的 offset 为 368801 的 message 在该 index 内的偏移量为 368796+5&#x3D;368801，所以这里要查找的相对 offset 为 5）。由于该文件采用的是稀疏索引的方式存储着相对 offset 及对应 message 物理偏移量的关系，所以直接找相对 offset 为 5 的索引找不到，这里同样利用二分法查找相对 offset 小于或者等于指定的相对 offset 的索引条目中最大的那个相对 offset，所以找到的是相对 offset 为 4 的这个索引。</li><li>根据找到的相对 offset 为 4 的索引确定 message 存储的物理偏移位置为 256。打开数据文件，从位置为 256 的那个地方开始顺序扫描直到找到 offset 为 368801 的那条 Message。</li></ol><p>这套机制是建立在 offset 为有序的基础上，利用 segment + 有序 offset + 稀疏索引 + 二分查找 + 顺序查找等多种手段来高效的查找数据！至此，消费者就能拿到需要处理的数据进行处理了。那每个消费者又是怎么记录自己消费的位置呢？在早期的版本中，消费者将消费到的 offset 维护 zookeeper 中，consumer 每间隔一段时间上报一次，这里容易导致重复消费，且性能不好！在新的版本中消费者消费到的 offset 已经直接维护在 kafk 集群的__consumer_offsets 这个 topic 中！</p>]]></content>
      
      
      <categories>
          
          <category> KafKa </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KafKa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KafKa实战问题</title>
      <link href="/KafKa/KafKa%E5%AE%9E%E6%88%98%E9%97%AE%E9%A2%98/"/>
      <url>/KafKa/KafKa%E5%AE%9E%E6%88%98%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="消息积压"><a href="#消息积压" class="headerlink" title="消息积压"></a>消息积压</h1><p>2024&#x2F;9&#x2F;23</p><p>消费者 逻辑 发生NPE导致–》 消费者不断重试消费—》发生消息积压—〉存在有的消息一直得不到消费</p><p>修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consume.exception.retry.count = 1 //表示失败后 重试消费1次</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/cgshao02/BLOG_images/main/image-20240923140747749.png?token=A5BZ7JLVJDUYNCL5PKYCQY3G6EDAK"></p>]]></content>
      
      
      <categories>
          
          <category> KafKa </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KafKa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据库慢查询</title>
      <link href="/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%85%A2%E6%9F%A5%E8%AF%A2/"/>
      <url>/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%85%A2%E6%9F%A5%E8%AF%A2/</url>
      
        <content type="html"><![CDATA[<h2 id="一、遇到的慢查询问题"><a href="#一、遇到的慢查询问题" class="headerlink" title="一、遇到的慢查询问题"></a>一、遇到的慢查询问题</h2><h3 id="1、表的索引区分度小导致的满查询"><a href="#1、表的索引区分度小导致的满查询" class="headerlink" title="1、表的索引区分度小导致的满查询"></a>1、表的索引区分度小导致的满查询</h3><p>为什么会命中不同的索引？</p><blockquote><p>索引的选择是优化器的工作，mysql的优化器选择索引时会极大的依赖“统计信息”。但是本质原因是原来的索引区分度很差</p></blockquote><h4 id="统计信息又是什么时候执行的？"><a href="#统计信息又是什么时候执行的？" class="headerlink" title="统计信息又是什么时候执行的？"></a>统计信息又是什么时候执行的？</h4><p>统计信息更新分为手动更新与自动更新。</p><p><strong>手动更新</strong></p><ol><li>ANALYZE table UPDATE HISTOGRAM ON <col_name> WITH <N> BUCKETS &#x2F;&#x2F;更新某个表的某个列的直方图</li><li>ANALYZE table DROP HISTOGRAM ON col_name &#x2F;&#x2F;删除某个表的直方图</li><li>ANALYZE table <table_name> &#x2F;&#x2F; 更新某个表中索引各个字段的cardinality</li></ol><p><strong>自动更新：</strong></p><ol><li>增删索引，加列， 清空表等涉及数据修改的DDL时；</li><li>后台线程发现表更新的记录超过表记录总数的1&#x2F;10时。</li></ol><h4 id="有触发过统计数据的日志吗？"><a href="#有触发过统计数据的日志吗？" class="headerlink" title="有触发过统计数据的日志吗？"></a>有触发过统计数据的日志吗？</h4><p>这是mysql的系统自发的行为，只能查看到最后一次更新的记录，没有历史日志。</p><p>DBA执行了analyze table后数据被覆盖了，忘记对analyze table命令前最后一次统计信息进行查看。</p><h4 id="这类满查询的注意事项"><a href="#这类满查询的注意事项" class="headerlink" title="这类满查询的注意事项"></a>这类满查询的注意事项</h4><p>（1）判断索引区分度的方法</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="keyword">distinct</span> <span class="keyword">column</span>) <span class="operator">/</span><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>)</span><br><span class="line"><span class="comment">-- 索引区分度要大于70% --</span></span><br></pre></td></tr></table></figure><p>（2）设置降级策略、设置限流</p><p>（3）添加系统告警</p>]]></content>
      
      
      <categories>
          
          <category> MySQL数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端开发 </tag>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo踩坑纪录</title>
      <link href="/Hexo%E6%95%99%E7%A8%8B/hexo%E8%B8%A9%E5%9D%91%E7%BA%AA%E5%BD%95/"/>
      <url>/Hexo%E6%95%99%E7%A8%8B/hexo%E8%B8%A9%E5%9D%91%E7%BA%AA%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h2 id="点击页面跳转-变成了-下载lianje"><a href="#点击页面跳转-变成了-下载lianje" class="headerlink" title="点击页面跳转 变成了 下载lianje"></a>点击页面跳转 变成了 下载lianje</h2><p>查阅之后发现问题所在是在config配置文件中的</p><p>｜permalink: :title&#x2F;</p><p>少写了最后的&#x2F;，就会出现以上情况</p><p>同时如果少写了：，点击文章就会变成跳转到第一篇博文，跳转不了别的文章。</p>]]></content>
      
      
      <categories>
          
          <category> Hexo教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java8新特性：Stream流</title>
      <link href="/Java%E5%9F%BA%E7%A1%80/Java8%E6%96%B0%E7%89%B9%E6%80%A7%EF%BC%9AStream%E6%B5%81/"/>
      <url>/Java%E5%9F%BA%E7%A1%80/Java8%E6%96%B0%E7%89%B9%E6%80%A7%EF%BC%9AStream%E6%B5%81/</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在 java 中，涉及到对数组、集合等集合类元素的操作时，通常我们使用的是循环的方式进行逐个遍历处理，或者使用 stream 流的方式进行处理。</p><h3 id="什么是-Stream？"><a href="#什么是-Stream？" class="headerlink" title="什么是 Stream？"></a>什么是 Stream？</h3><p>Stream（流）是一个来自数据源的元素队列并支持聚合操作,流在管道中传输， 并且可以在管道的节点上进行处理， 比如筛选， 排序，聚合等。 Stream（流）的组成包含:元素、数据源、聚合操作、内部迭代、Pipelining等。</p><h3 id="创建-Stream-流"><a href="#创建-Stream-流" class="headerlink" title="创建 Stream 流"></a>创建 Stream 流</h3><p>1）stream()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Stream&lt;String&gt; stream = stringList.stream();</span><br></pre></td></tr></table></figure><p>2）parallelStream()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Stream&lt;String&gt; stringStream = stringList.parallelStream();</span><br></pre></td></tr></table></figure><h3 id="Stream-流常用操作"><a href="#Stream-流常用操作" class="headerlink" title="Stream 流常用操作"></a>Stream 流常用操作</h3><p>1）forEach </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stringList.forEach(System.out::println);</span><br></pre></td></tr></table></figure><p>2）map</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stringList.stream().map(i-&gt;i.equals(<span class="string">&quot;juejin&quot;</span>));</span><br></pre></td></tr></table></figure><p>3）filter</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stringList.stream().filter(i-&gt;i.equals(<span class="string">&quot;juejin&quot;</span>));</span><br></pre></td></tr></table></figure><p>4）limit</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">integerList.stream().limit(<span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>5）skip</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">integerList.stream().skip(<span class="number">5</span>).limit(<span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>6）distinct</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">integerList.stream().distinct().collect(Collectors.toList());</span><br></pre></td></tr></table></figure><p>7）sorted</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">integerList.stream().sorted();</span><br></pre></td></tr></table></figure><p>8）sorted(Comparator com)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">integerList.stream().sorted(Comparator.comparing(Integer::intValue));</span><br></pre></td></tr></table></figure><p>9）Collectors 收集器</p><p><img src="https://pic.yupi.icu/5563/202401242026474.png"></p><ul><li>恒等处理 Collectors</li></ul><p>所谓<strong>恒等处理</strong>，指的就是Stream的元素在经过Collector函数处理前后完全不变，例如toList()操作，只是最终将结果从Stream中取出放入到List对象中，并没有对元素本身做任何的更改处理</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">list.stream().collect(Collectors.toList());</span><br><span class="line">list.stream().collect(Collectors.toSet());</span><br><span class="line">list.stream().collect(Collectors.toCollection());</span><br></pre></td></tr></table></figure><ul><li>归约汇总 Collectors</li></ul><p>对于<strong>归约汇总</strong>类的操作，Stream流中的元素逐个遍历，进入到Collector处理函数中，然后会与上一个元素的处理结果进行合并处理，并得到一个新的结果，以此类推，直到遍历完成后，输出最终的结果</p><table><thead><tr><th>counting</th><th>统计流中的元素个数</th></tr></thead><tbody><tr><td>summingInt</td><td>计算流中指定int字段的累加总和。针对不同类型的数字类型，有不同的方法，比如summingDouble等</td></tr><tr><td>averagingInt</td><td>计算流中指定int字段的平均值。针对不同类型的数字类型，有不同的方法，比如averagingLong等</td></tr><tr><td>joining</td><td>将流中所有元素（或者元素的指定字段）字符串值进行拼接，可以指定拼接连接符，或者首尾拼接字符</td></tr><tr><td>maxBy</td><td>根据给定的比较器，选择出值最大的元素</td></tr><tr><td>minBy</td><td>根据给定的比较器，选择出值最小的元素</td></tr></tbody></table><ul><li><p>分组分区 Collectors</p></li><li><ul><li>仅仅是做一个常规的数据分组操作时，可以仅传入一个分组函数即可</li></ul></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">groupBySubCompany</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// 按照子公司维度将员工分组</span></span><br><span class="line">    Map&lt;String, List&lt;Employee&gt;&gt; resultMap =</span><br><span class="line">            getAllEmployees().stream()</span><br><span class="line">                    .collect(Collectors.groupingBy(Employee::getSubCompany));</span><br><span class="line">    System.out.println(resultMap);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><ul><li>如果不仅需要分组，还需要对分组后的数据进行处理的时候，则需要同时给定分组函数以及值收集器</li></ul></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">groupAndCaculate</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// 按照子公司分组，并统计每个子公司的员工数</span></span><br><span class="line">    Map&lt;String, Long&gt; resultMap = getAllEmployees().stream()</span><br><span class="line">            .collect(Collectors.groupingBy(Employee::getSubCompany,</span><br><span class="line">                    Collectors.counting()));</span><br><span class="line">    System.out.println(resultMap);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>简而言之，Stream API 提供了一种高效且易于使用的处理数据的方式。让程序员写出高效率、干净、简洁的代码。</p>]]></content>
      
      
      <categories>
          
          <category> Java基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java基础 </tag>
            
            <tag> 后端开发 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
