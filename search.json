[{"title":"golang（入门）初始化","path":"/GO/golang（入门）初始化/","content":"在 Go 语言中，我们常用的数据结构有在 Go 语言中，你可以初始化数据结构，例如数组、切片、结构体、指针、map 等。以下是这些数据结构初始化的实例： 数组初始化 package mainimport fmtfunc main() var arr [5]int = [5]int1, 2, 3, 4, 5 fmt.Println(arr) 切片初始化： package main import fmt func main() var s []int = []int1, 2, 3, 4, 5 fmt.Println(s) 结构体初始化： package main import fmt type Person struct Name string Age int func main() var p Person = PersonJohn Doe, 30 fmt.Println(p) 指针初始化： package mainimport fmtfunc main() var x int = 10 var p *int = x fmt.Println(*p) Map 初始化： package mainimport fmtfunc main() var m map[string]int = map[string]intone: 1, two: 2 fmt.Println(m) 请注意，这些示例都使用了 var 关键字进行初始化，但在 Go 语言中，你还可以使用 “:=” 操作符进行简短声明和初始化，例如： s := []int1, 2, 3, 4, 5 p := PersonJohn Doe, 30 m := map[string]intone: 1, two: 2","tags":["GO"],"categories":["GO"]},{"title":"redis实践","path":"/redis/redis实践/","content":"一、背景 Redis 最佳场景 小容量、热数据最佳实践选择合适的数据结构 杜绝BigKey HotKey 避免复杂操作 不要过度使用 二、 Redis 大key大value拆分方案业务场景中经常会有各种大Value多Value的情况， 比如： 1： 单个简单的key存储的value很大 2： hash， set，zset，list 中存储过多的元素（以万为单位） 3：一个集群存储了上亿的key，Key 本身过多也带来了更多的空间占用 为什拆分? 由于redis是单线程运行的，如果一次操作的value很大会对整个redis的响应时间造成负面影响1、单个简单的key存储的value很大i： 该对象需要每次都整存整取 可以尝试将对象分拆成几个key-value， 使用multiGet获取值，这样分拆的意义在于分拆单次操作的压力，将操作压力平摊到多个redis实例中，降低对单个redis的IO影响； ii： 该对象每次只需要存取部分数据 可以像第一种做法一样，分拆成几个key-value， 也可以将这个存储在一个hash中，每个field代表一个具体的属性，使用hget,hmget来获取部分的value，使用hset，hmset来更新部分属性 2、hash, set , zset , list中存储过多的元素类似于场景一种的第一个做法，可以将这些元素分拆。 以hash为例，原先的正常存取流程是 hget(hashKey, field) ; hset(hashKey, field, value) 现在，固定一个桶的数量，比如 10000， 每次存取的时候，先在本地计算field的hash值，模除 10000， 确定了该field落在哪个key上。 newHashKey = hashKey + ( hash(field) % 10000）; hset (newHashKey, field, value) ; hget(newHashKey, field) set, zset, list 也可以类似上述做法 但有些不适合的场景，比如，要保证 lpop 的数据的确是最早push到list中去的，这个就需要一些附加的属性，或者是在 key的拼接上做一些工作（比如list按照时间来分拆）。 3、一个集群存储了上亿的key如果key的个数过多会带来更多的内存空间占用， i：key本身的占用（每个key 都会有一个Category前缀） ii：集群模式中，服务端需要建立一些slot2key的映射关系，这其中的指针占用在key多的情况下也是浪费巨大空间 这两个方面在key个数上亿的时候消耗内存十分明显（Redis 3.2及以下版本均存在这个问题，4.0有优化）； 所以减少key的个数可以减少内存消耗，可以参考的方案是转Hash结构存储，即原先是直接使用Redis String 的结构存储，现在将多个key存储在一个Hash结构中，具体场景参考如下： 一： key 本身就有很强的相关性，比如多个key 代表一个对象，每个key是对象的一个属性，这种可直接按照特定对象的特征来设置一个新Key——Hash结构， 原先的key则作为这个新Hash 的field。 举例说明： 原先存储的三个key ， user.zhangsan-id = 123; user.zhangsan-age = 18; user.zhangsan-country = china; 这三个key本身就具有很强的相关特性，转成Hash存储就像这样 key = user.zhangsan field:id = 123; field:age = 18; field:country = china; 即redis中存储的是一个key ：user.zhangsan， 他有三个 field， 每个field + key 就对应原先的一个key。 二： key 本身没有相关性，预估一下总量，采取和上述第二种场景类似的方案，预分一个固定的桶数量 比如现在预估key 的总数为 2亿，按照一个hash存储 100个field来算，需要 2亿 / 100 = 200W 个桶 (200W 个key占用的空间很少，2亿可能有将近 20G ) 原先比如有三个key （userId） ： 123456789 , 987654321， 678912345 现在按照200W 固定桶分就是先计算出桶的序号 hash(123456789) % 200W ， 这里最好保证这个 hash算法的值是个正数，否则需要调整下模除的规则； 这样算出三个key 的桶分别是 1 ， 2， 2 ， 所以存储的时候调用API 原先 set (realKey, value) === 现在 hset （bucketKey，realKey， value ）， 读取的时候使用原先 get(realKey) === 现在hget（bucketKey， realKey） key1 : hset（userid-bucket-1,123456789,value ） hget（userid-bucket-1, 123456789） key2:hset (userid-bucket-2, 987654321,value ) hget（userid-bucket-2,987654321） key3: hset（userid-bucket-2, 678912345, value) hget（userid-bucket-2, 678912345） （这里 bucket key 为了标识出来意义， 加了个前缀 userid-bucket， 不影响整体逻辑，业务自行判断） 注意两个地方：1，hash 取模对负数的处理； 2，预分桶的时候， 一个hash 中存储的值最好不要超过 512 ，100 左右较为合适","tags":["redis"],"categories":["redis"]},{"title":"golang(入门)struct & interface & 类型断言","path":"/GO/golang(入门)struct & interface & 类型断言/","content":"GoLang入门一、 structpackage mainimport fmttype person struct name string\tage uint\taddr addresstype address struct province string\tcity stringfunc main() per := person age: 12, name: zuolong, addr: address province: beijing, city: beijing, , fmt.Println(per)\tfmt.Println(per.addr.province) 二、 interface 接口Golang 中的接口是一种抽象数据类型，Golang 中接口定义了对象的行为规范，只定义规范 不实现。接口中定义的规范由具体的对象来实现。 通俗的讲接口就一个标准，它是对一个对象的行为和规范进行约定，约定实现接口的对象必 须得按照接口的规范。 package main import fmt // 定义电脑usb接口type Usber2 interface start()\tstop() // 定义电脑结构体type Computer struct // 定义电脑usb工作方式func (c Computer) Work(u Usber2) u.start()\tu.stop() // 定义手机type Phone2 struct func (p Phone2) start() fmt.Println(手机连接成功) func (p Phone2) stop() fmt.Println(手机断开连接) // 定义相机type Cermail struct func (c Cermail) start() fmt.Println(相机连接成功) func (c Cermail) stop() fmt.Println(相机断开连接) func main() // 分别定义变量\tp1 := Phone2\tc := Cermail\tcomputer := Computer\t// 手机连接电脑\tcomputer.Work(p1) // 手机连接成功 手机连接成功\t// 相机连接电脑\tcomputer.Work(c) // 相机连接成功 相机断开连接 空接口Golang 中的接口可以不定义任何方法，没有定义任何方法的接口就是空接口。 空接口表示没有任何约束，因此任何类型变量都可以实现空接口。 var a interface // 定义变量a为空接口a = 你好 golangfmt.Printf(值：%v 类型：%T , a, a) // 值：你好 golang 类型：stringa = 10fmt.Printf(值：%v 类型：%T , a, a) // 值：10 类型：inta = truefmt.Printf(值：%v 类型：%T , a, a) // 值：true 类型：bool 空接口做函数参数// 空接口作为函数参数，代表该函数接收任意参数func test(i interface) fmt.Printf(值：%v 类型：%T , i, i) func main() // 随意传入参数\ttest(1) // 值：1 类型：int\ttest(阿西吧) // 值：1 类型：int\tvar b = make(map[string]string)\tb[哈哈] = 嘻嘻\ttest(b) // 值：map[哈哈:嘻嘻] 类型：map[string]string\tvar c = []int1, 2, 3\ttest(c) // 值：[1 2 3] 类型：[]int map值实现空接口 // 空接口作为map，代表可以保存任意值var e = make(map[string]interface)e[username] = 张三e[age] = 12fmt.Println(e) // map[age:12 username:张三] var f = make(map[interface]interface)f[123] = 456f[test] = 123f[fdas] = ghufmt.Println(f) // map[123:456 fdas:ghu test:123] 切片实现空接口// 切片空接口var g = []interface1, 张三, truefmt.Println(g) // [1 张三 true] 类型断言一个接口的值（简称接口值）是由一个具体类型和具体类型的值两部分组成的。这两部分分别称为接口的动态类型和动态值。 如果我们想要判断空接口中值的类型，那么这个时候就可以使用类型断言，其语法格式：x.(T) 其中： x : 表示类型为 interface{} 的变量 T : 表示断言 x 可能是的类型。 该语法返回两个参数，第一个参数是 x 转化为 T 类型后的变量，第二个值是一个布尔值，若为 true 则表示断言成功，为 false 则表示断言失败。 1. 断言的基础使用// 断言基础使用var h interfaceh = 张三v, ok := h.(string)if ok fmt.Println(v) // 张三 else fmt.Println(不是string类型) 2. 多次断言配合 swtich 使用package main import fmt // 多次断言配合switch使用，另外类型.(type)只能配合switch使用func vifType(s interface) switch s.(type) case string: fmt.Println(string)\tcase int: fmt.Println(int)\tcase bool: fmt.Println(bool)\tdefault: fmt.Println(no search) func main() // 多次断言配合switch使用\tvifType(abcd) // string\tvifType(123) // int\tvifType(true) // bool\tvifType(int64(123)) // no search 3. 注意因为空接口可以存储任意类型值的特点，所以空接口在 Go 语言中的使用十分广泛。 关于接口需要注意的是：只有当有两个或两个以上的具体类型必须以相同的方式进行处理时才需要定义接口。不要为了接口而写接口，那样只会增加不必要的抽象，导致不必要的运行时损耗。","categories":["GO"]},{"title":"分布式锁","path":"/实际开发/分布式锁/","content":"TODO分布式锁实践—待总结 基于Redis Cluster 模式下的分布式锁的实现基于setnx锁实现加锁setnx k vkey做为锁的唯一标识,当setnx返回1时,说明原本key不存在,该线程成功得到了锁;如果返回的结果为0,则说明key已经存在,线程获取失败 解锁del key 通过制定key的过期时间,让锁到期后自动释放 set key value [EX seconds] [PX milliseconds] [NX|XX]EX seconds：设置键key的过期时间，单位时秒; PX milliseconds：设置键key的过期时间，单位时毫秒; NX：只有键key不存在的时候才会设置key的值； XX：只有键key存在的时候才会设置key的值。 通过 set (key, value, EX a,NX） 取代setnx 来实现原子加锁(自动释放锁)操作。 基于Redis实现分布式锁的一些问题在高并发下的分布式锁实现中，key的过期肯定不能设置的太长，否则会影响后续线程持有该锁； 但是如果设置过期时间很短，直到key过期，持有该锁的线程还未执行完任务；接着下一个线程获取到该锁，这时候前一个线程执行完成后触发del释放该锁，而这把锁这个时候其实是另外一个线程持有； 获取锁是非阻塞的，无论成功还是失败就直接返回； 锁公平问题，所有等待线程同时发起获取锁命令操作。 针对上述问题需要另外服务来保证实现： 守护线程：如果某个线程在expire时间内，还未执行完成，守护线程自动expire一个新过期时间，直到该线程执行完成或释放； 释放验证：线程加锁前通过线程ID和Key Value匹配，释放前通过两者判断是否一致，一致再释放该锁，避免错误释放其他线程执行时持有相同的锁(原子性可以通过lua脚本来实现)； 阻塞锁： 通过while true之类的机制去阻塞代码实现； 公平锁：可以通过将所有等待线程放入同一个队列来实现。 可重入性 Java中的Lock对象以及Synchronized关键字语块都可具有可重入性，可以实现同一个线程中共用同一把锁；避免死锁发生的可能。 而在Redis上述实现中则没有相应的功能，如果业务上需要，则需在业务代码中实现其逻辑。 安全性 Redis Cluster 在master异常情况下，会发生主从切换，而主从是异步复制，极大可能导致数据丢失，从而导致锁的失效。这块安全性方面Redis Cluster 无法保证。 但是Redis 作者实现了基于多节点的高可用分布式锁的算法 RedLock。有兴趣的可以了解一下。","tags":["实际开发"],"categories":["实际开发"]},{"title":"Spring循环依赖","path":"/实际开发/Spring循环依赖/","content":"Spring循环依赖12.19 遇到Spring循环依赖的问题 — TODO 通过 @Lazy解决 据说循环依赖问题还可以通过, set注入. @PostConstract 等方式解决 todo","categories":["实际开发"]},{"title":"fullGC","path":"/实际开发/fullGC/","content":"fullGC一、fullGC 产生的原因分析 [] 查过几个案例,大对象导致full GC的情况比较多, 表现为通过SQL之类查询出了大对象信息,导致full GC ,然后系统主机STW 无法提供服务 简单分析 ① 原因分析 不断生成大对象，大对象不进eden，直接进old，导致fgc，可以通过对象分析来看 不断生成小对象，小对象塞满了eden，触发ygc，进到old，满了导致fgc。现象是ygc次数非常多，内存占用增长很快 元空间增长快。此时需要检查是不是有不断生成的类，例如星脉的Java就是使用了动态生成类加载器和动态生成类的能力，不控制容易打爆元空间 另外还有一种堆外内存增长过快，不会触发fgc，但可能会导致机器内存占满的情况 二、参考文档JVM出现连续的FullGC该怎么办","categories":["实际开发"]},{"title":"范型","path":"/Java基础/范型/","content":"一、引入没有泛型会怎样先看下面这段代码：我们实现两个能够设置点坐标的类，分别设置 Integer 类型的点坐标和 Float 类型的点坐标： //设置Integer类型的点坐标class IntegerPoint private Integer x ; // 表示X坐标 private Integer y ; // 表示Y坐标 public void setX(Integer x) this.x = x ; public void setY(Integer y) this.y = y ; public Integer getX() return this.x ; public Integer getY() return this.y ; //设置Float类型的点坐标class FloatPoint private Float x ; // 表示X坐标 private Float y ; // 表示Y坐标 public void setX(Float x) this.x = x ; public void setY(Float y) this.y = y ; public Float getX() return this.x ; public Float getY() return this.y ; 那现在有个问题：大家有没有发现，他们除了变量类型不一样，一个是 Integer 一个是 Float 以外，其它并没有什么区别！那我们能不能合并成一个呢？答案是可以的，因为 Integer 和 Float 都是派生自 Object 的，我们用下面这段代码代替： class ObjectPoint private Object x ; private Object y ; public void setX(Object x) this.x = x ; public void setY(Object y) this.y = y ; public Object getX() return this.x ; public Object getY() return this.y ; 即全部都用 Object 来代替所有的子类；在使用的时候是这样的： ObjectPoint integerPoint = new ObjectPoint();integerPoint.setX(new Integer(100));Integer integerX=(Integer)integerPoint.getX(); 在设置的时候，使用 new Integer(100) 来新建一个 Integer integerPoint.setX(new Integer(100)); 然后在取值的时候，进行强制转换： Integer integerX=(Integer)integerPoint.getX(); 由于我们设置的时候，是设置的 Integer，所以在取值的时候，强制转换是不会出错的。同理，FloatPoint 的设置和取值也是类似的，代码如下： ObjectPoint floatPoint = new ObjectPoint();floatPoint.setX(new Float(100.12f));Float floatX = (Float)floatPoint.getX(); 但问题来了：注意，注意，我们这里使用了强制转换，我们这里 setX（）和 getX（）写得很近，所以我们明确的知道我们传进去的是 Float 类型，那如果我们记错了呢？比如我们改成下面这样，编译时会报错吗： ObjectPoint floatPoint = new ObjectPoint();floatPoint.setX(new Float(100.12f));String floatX = (String)floatPoint.getX(); 不会！！！我们问题的关键在于这句： String floatX = (String)floatPoint.getX(); 强制转换时，会不会出错。因为编译器也不知道你传进去的是什么，而 floatPoint.getX() 返回的类型是 Object，所以编译时，将 Object 强转成 String 是成立的。必然不会报错。而在运行时，则不然，在运行时，floatPoint 实例中明明传进去的是 Float 类型的变量，非要把它强转成 String 类型，肯定会报类型转换错误的！那有没有一种办法在编译阶段，即能合并成同一个，又能在编译时检查出来传进去类型不对呢？当然，这就是泛型。下面我们将对泛型的写法和用法做一一讲解。 二、各种泛型定义及使用1、泛型类定义及使用我们先看看泛型的类是怎么定义的： //定义class PointT// 此处可以随便写标识符号 private T x ; private T y ; public void setX(T x)//作为参数 this.x = x ; public void setY(T y) this.y = y ; public T getX()//作为返回值 return this.x ; public T getY() return this.y ; ;//IntegerPoint使用PointInteger p = new PointInteger() ; p.setX(new Integer(100)) ; System.out.println(p.getX()); //FloatPoint使用PointFloat p = new PointFloat() ; p.setX(new Float(100.12f)) ; System.out.println(p.getX()); 先看看运行结果： 100.12 从结果中可以看到，我们实现了开篇中 IntegerPoint 类和 FloatPoint 类的效果。下面来看看泛型是怎么定义及使用的吧。 （1）、定义泛型：Point首先，大家可以看到 Point，即在类名后面加一个尖括号，括号里是一个大写字母。这里写的是 T，其实这个字母可以是任何大写字母，大家这里先记着，可以是任何大写字母，意义是相同的。（2）类中使用泛型这个 T 表示派生自 Object 类的任何类，比如 String,Integer,Double 等等。这里要注意的是，T 一定是派生于 Object 类的。为方便起见，大家可以在这里把 T 当成 String, 即 String 在类中怎么用，那 T 在类中就可以怎么用！所以下面的：定义变量，作为返回值，作为参数传入的定义就很容易理解了。 //定义变量private T x ; //作为返回值public T getX() return x ; //作为参数public void setX(T x) this.x = x ; （3）使用泛型类下面是泛型类的用法： 首先，是构造一个实例： PointString p = new PointString() ; 这里与普通构造类实例的不同之点在于，普通类构造函数是这样的：Point p = new Point() ;而泛型类的构造则需要在类名后添加上 ，即一对尖括号，中间写上要传入的类型。因为我们构造时，是这样的：class Point, 所以在使用的时候也要在 Point 后加上类型来定义 T 代表的意义。然后在 getVar（）和 setVar（）时就没有什么特殊的了，直接调用即可。从上面的使用时，明显可以看出泛型的作用，在构造泛型类的实例的时候： (4）使用泛型实现的优势相比我们开篇时使用 Object 的方式，有两个优点：（1）、不用强制转换 (2)、在 setVar() 时如果传入类型不对，编译时会报错 可以看到，当我们构造时使用的是 String, 而在 setVar 时，传进去 Integer 类型时，就会报错。而不是像 Object 实现方式一样，在运行时才会报强制转换错误。 2、多泛型变量定义及字母规范（1）、多泛型变量定义上在我们只定义了一个泛型变量 T，那如果我们需要传进去多个泛型要怎么办呢？只需要在类似下面这样就可以了： class MorePointT,U//新添加的泛型变量 U 用法与 T 是一样的。 （2）、字母规范在定义泛型类时，我们已经提到用于指定泛型的变量是一个大写字母： class PointT ………… 当然不是的！！！！任意一个大写字母都可以。他们的意义是完全相同的，但为了提高可读性，大家还是用有意义的字母比较好，一般来讲，在不同的情境下使用的字母意义如下： E — Element，常用在 java Collection 里，如：List,Iterator,Set K,V — Key，Value，代表 Map 的键值对 N — Number，数字 T — Type，类型，如 String，Integer 等等 如果这些还不够用，那就自己随便取吧，反正 26 个英文字母呢。再重复一遍，使用哪个字母是没有特定意义的！只是为了提高可读性！！！！ 3、泛型接口定义及使用在接口上定义泛型与在类中定义泛型是一样的，代码如下： interface InfoT // 在接口上定义泛型 public T getVar() ; // 定义抽象方法，抽象方法的返回值就是泛型类型 public void setVar(T x); 与泛型类的定义一样，也是在接口名后加尖括号；（1）、使用方法一：非泛型类但是在使用的时候，就出现问题了，我们先看看下面这个使用方法： class InfoImpl implements InfoString\t// 定义泛型接口的子类 private String var ; // 定义属性 public InfoImpl(String var) // 通过构造方法设置属性内容 this.setVar(var) ; @Override public void setVar(String var) this.var = var ; @Override public String getVar() return this.var ; public class GenericsDemo24 public void main(String arsg[]) InfoImpl i = new InfoImpl(harvic); System.out.println(i.getVar()) ; ; 首先，先看 InfoImpl 的定义： class InfoImpl implements InfoString ………… 要清楚的一点是 InfoImpl 不是一个泛型类！因为他类名后没有 ！然后在在这里我们将 Info 中的泛型变量 T 定义填充为了 String 类型。所以在重写时 setVar（）和 getVar（）时，IDE 会也我们直接生成 String 类型的重写函数。最后在使用时, 没什么难度，传进去 String 类型的字符串来构造 InfoImpl 实例，然后调用它的函数即可。 public class GenericsDemo24 public void main(String arsg[]) InfoImpl i = new InfoImpl(harvic); System.out.println(i.getVar()) ; ; （2）、使用方法二：泛型类 在方法一中，我们在类中直接把 Info 接口给填充好了，但我们的类，是可以构造成泛型类的，那我们利用泛型类来构造填充泛型接口会是怎样呢？ interface InfoT // 在接口上定义泛型\tpublic T getVar() ;\t// 定义抽象方法，抽象方法的返回值就是泛型类型\tpublic void setVar(T var);class InfoImplT implements InfoT\t// 定义泛型接口的子类\tprivate T var ; // 定义属性\tpublic InfoImpl(T var) // 通过构造方法设置属性内容 this.setVar(var) ; public void setVar(T var) this.var = var ; public T getVar() return this.var ;\tpublic class GenericsDemo24\tpublic static void main(String arsg[]) InfoImplString i = new InfoImplString(harvic); System.out.println(i.getVar()) ;\t; 最关键的是构造泛型类的过程： class InfoImplT implements InfoT\t// 定义泛型接口的子类\tprivate T var ; // 定义属性\tpublic InfoImpl(T var) // 通过构造方法设置属性内容 this.setVar(var) ; public void setVar(T var) this.var = var ; public T getVar() return this.var ; 在这个类中，我们构造了一个泛型类 InfoImpl，然后把泛型变量 T 传给了 Info，这说明接口和泛型类使用的都是同一个泛型变量。然后在使用时，就是构造一个泛型类的实例的过程，使用过程也不变。 public class GenericsDemo24\tpublic static void main(String arsg[]) InfoString i = new InfoImplString(harvic); System.out.println(i.getVar()) ;\t; 使用泛型类来继承泛型接口的作用就是让用户来定义接口所使用的变量类型，而不是像方法一那样，在类中写死。那我们稍微加深点难度，构造一个多个泛型变量的类，并继承自 Info 接口： class InfoImplT,K,U implements InfoU\t// 定义泛型接口的子类 private U var ; private T x; private K y; public InfoImpl(U var) // 通过构造方法设置属性内容 this.setVar(var) ; public void setVar(U var) this.var = var ; public U getVar() return this.var ; 在这个例子中，我们在泛型类中定义三个泛型变量 T,K,U 并且把第三个泛型变量 U 用来填充接口 Info。所以在这个例子中 Info 所使用的类型就是由 U 来决定的。使用时是这样的：泛型类的基本用法，不再多讲, 代码如下： public class GenericsDemo24 public void main(String arsg[]) InfoImplInteger,Double,String i = new InfoImplInteger,Double,String(harvic); System.out.println(i.getVar()) ; 4、泛型函数定义及使用public class StaticFans //静态函数 public static T void StaticMethod(T a) Log.d(harvic,StaticMethod: +a.toString()); //普通函数 public T void OtherMethod(T a) Log.d(harvic,OtherMethod: +a.toString()); 上面分别是静态泛型函数和常规泛型函数的定义方法，与以往方法的唯一不同点就是在返回值前加上 来表示泛型变量。其它没什么区别。使用方法如下： //静态方法StaticFans.StaticMethod(adfdsa);//使用方法一StaticFans.StringStaticMethod(adfdsa);//使用方法二 //常规方法StaticFans staticFans = new StaticFans();staticFans.OtherMethod(new Integer(123));//使用方法一staticFans.IntegerOtherMethod(new Integer(123));//使用方法二 结果如下： adfsaadfsa 首先，我们看静态泛型函数的使用方法： StaticFans.StaticMethod(adfdsa);//使用方法一StaticFans.StringStaticMethod(adfdsa);//使用方法二 从结果中我们可以看到，这两种方法的结果是完全一样的，但他们还有些区别的，区别如下：方法一，可以像普通方法一样，直接传值，任何值都可以（但必须是派生自 Object 类的类型，比如 String,Integer 等），函数会在内部根据传进去的参数来识别当前 T 的类别。但尽量不要使用这种隐式的传递方式，代码不利于阅读和维护。因为从外观根本看不出来你调用的是一个泛型函数。方法二，与方法一不同的地方在于，在调用方法前加了一个 来指定传给 T 的值，如果加了这个 String 来指定参数的值的话，那 StaticMethod（）函数里所有用到的 T 类型也就是强制指定了是 String 类型。这是我们建议使用的方式。同样，常规泛型函数的使用也有这两种方式： StaticFans staticFans = new StaticFans();staticFans.OtherMethod(new Integer(123));//使用方法一staticFans.IntegerOtherMethod(new Integer(123));//使用方法二 可以看到，与平常一样，先创建类的实例，然后调用泛型函数。方法一，隐式传递了 T 的类型，与上面一样，不建议这么做。方法二，显示将 T 赋值为 Integer 类型，这样 OtherMethod（T a）传递过来的参数如果不是 Integer 那么编译器就会报错。进阶：返回值中存在泛型上面我们的函数中，返回值都是 void，但现实中不可能都是 void，有时，我们需要将泛型变量返回，比如下面这个函数： public static T ListT parseArray(String response,ClassT object) ListT modelList = JSON.parseArray(response, object); return modelList; 函数返回值是 List 类型。至于传入参数 Class object 的意义，我们下面会讲。这里也就是想通过这个例子来告诉大家，泛型变量其实跟 String,Integer，Double 等等的类的使用上没有任何区别，T 只是一个符号，可以代表 String,Integer，Double…… 这些类的符号，在泛型函数使用时，直接把 T 看到 String,Integer，Double…… 中的任一个来写代码就可以了。唯一不同的是，要在函数定义的中在返回值前加上 T 标识泛型； 5、其它用法: Class T 类传递及泛型数组（1）、使用 Class T 传递泛型类 Class 对象有时，我们会遇到一个情况，比如，我们在使用 JSON 解析字符串的时候，代码一般是这样的 public static ListSuccessModel parseArray(String response) ListSuccessModel modelList = JSON.parseArray(response, SuccessModel.class); return modelList; 其中 SuccessModel 是自定义的解析类，代码如下，其实大家不用管 SuccessModel 的定义，只考虑上面的那段代码就行了。写出来 SuccessModel 的代码，只是不想大家感到迷惑，其实，这里只是 fastJson 的基本用法而已。这段代码的意义就是根据 SuccessModel 解析出 List 的数组。 public class SuccessModel private boolean success; public boolean isSuccess() return success; public void setSuccess(boolean success) this.success = success; 那现在，我们把下面这句组装成一个泛型函数要怎么来做呢? public static ListSuccessModel parseArray(String response) ListSuccessModel modelList = JSON.parseArray(response, SuccessModel.class); return modelList; 首先，我们应该把 SuccessModel 单独抽出来做为泛型变量，但 parseArray（）中用到的 SuccessModel.class 要怎么弄呢？先来看代码： public static T ListT parseArray(String response,ClassT object) ListT modelList = JSON.parseArray(response, object); return modelList; 注意到，我们用的 Class object 来传递类的 class 对象，即我们上面提到的 SuccessModel.class。这是因为 Class 也是一泛型，它是传来用来装载类的 class 对象的，它的定义如下： public final class ClassT implements Serializable ………… 通过 Class 来加载泛型的 Class 对象的问题就讲完了，下面来看看泛型数组的使用方法吧。（2）、定义泛型数组在写程序时，大家可能会遇到类似 String[] list = new String[8]; 的需求，这里可以定义 String 数组，当然我们也可以定义泛型数组，泛型数组的定义方法为 T[]，与 String[] 是一致的，下面看看用法： //定义public static T T[] fun1(T...arg) // 接收可变参数 return arg ; // 返回泛型数组 //使用public static void main(String args[]) Integer i[] = fun1(1,2,3,4,5,6) ; Integer[] result = fun1(i) ; 我们先看看 定义时的代码： public static T T[] fun1(T...arg) // 接收可变参数 return arg ; // 返回泛型数组","categories":["Java基础"]},{"title":"ScheduledExecutorService常用方法","path":"/实际开发/Java基础/ScheduledExecutorService常用方法/","content":"ScheduledExecutorService 使用（一）定时任务的几种方案介绍使用 java 做一个后台的定时任务，方案可以有如下几种：1、JDK 自带的 Timer2、JDK1.5+ 新增的 ScheduledExecutorService3、Quartz ：简单而强大的 JAVA 作业调度框架，可以支持动态的 Cron 语法 （二）ScheduledExecutorService 介绍ScheduledExecutorService 出自 java 的并发包：java.util.concurrent，这个包下的很多需要探索。今天我们就来使用一下 ScheduledExecutorService 实现定时任务： ScheduledThreadPoolExecutor 继承自 ThreadPoolExecutor。它主要用来在给定的延迟之后运 行任务，或者定期执行任务。ScheduledThreadPoolExecutor 的功能与 Timer 类似，但 ScheduledThreadPoolExecutor 功能更强大、更灵活。Timer 对应的是单个后台线程，而 ScheduledThreadPoolExecutor 可以在构造函数中指定多个对应的后台线程数。 （三）ScheduledExecutorService 的使用ScheduledExecutorService 包括三个方法：schedule()、scheduleAtFixedRate()、scheduleWithFixedDelay()。 3.1 schedule() 使用先上代码： public static void main(String[] args) scheduleTest(); //scheduleFixedRate(); //scheduleWithFixedDelay();//schedule()的用法,可以对任务进行延迟处理static void scheduleTest() ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(1); //获取当前时间 long cur = System.currentTimeMillis(); scheduledExecutorService.schedule(new Runnable() @Override public void run() //当前任务执行时候，对应的时间 System.out.println(延迟了+(System.currentTimeMillis() - cur) + ms); ,3000, TimeUnit.MILLISECONDS); scheduledExecutorService.shutdown(); 可以看到 schedule() 方法下需要三个参数，分别是：一个 runnable 线程，延迟时间，时间单位。 C:\\Program Files\\Java\\延迟了3013msProcess finished with exit code 0 实际情况下，run 方法体中代码执行时间延迟了 3s 执行，多出来的是建立线程花费的时间。此时再加入这样的条件，如果线程中的任务需要一定的执行时间，如果再有一个延迟任务执行呢？代码如下 public static void main(String[] args) scheduleTest(); //scheduleFixedRate(); //scheduleWithFixedDelay();//schedule()的用法,可以对任务进行延迟处理static void scheduleTest() ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(1); //获取当前时间 long cur = System.currentTimeMillis(); scheduledExecutorService.schedule(new Runnable() @Override public void run() //当前任务执行时候，对应的时间 System.out.println(第一次延迟了+(System.currentTimeMillis() - cur) + ms); //任务执行需要时间 try Thread.sleep(3000); catch (InterruptedException e) e.printStackTrace(); ,3000, TimeUnit.MILLISECONDS); scheduledExecutorService.schedule(new Runnable() @Override public void run() //当前任务执行时候，对应的时间 System.out.println(第二次延迟了+(System.currentTimeMillis() - cur) + ms); //任务执行需要时间 try Thread.sleep(3000); catch (InterruptedException e) e.printStackTrace(); ,3000, TimeUnit.MILLISECONDS); scheduledExecutorService.shutdown(); 线程池中线程数量指定为 1，运行结果如下： 第一次延迟了3017ms第二次延迟了6022msProcess finished with exit code 0 可以看到第二个任务需要等到第一个任务执行完成以后才进行执行。等待时间为延迟时间 + 任务时间。这时，如果将线程数目改为 2，再次执行： 第一次延迟了3007ms第二次延迟了3007msProcess finished with exit code 0 可以看到 ScheduledExecutorService 的强大之处了。可以在构造函数中指定多个对应的后台线程数，并发的运行延迟任务。 3.2 scheduleFixedRate() 使用 //scheduleAtFixedRate()的用法//在schedule下，加了周期运行的条件。static void scheduleFixedRate() ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(1); //获取当前时间 long cur = System.currentTimeMillis(); scheduledExecutorService.scheduleAtFixedRate(new Runnable() @Override public void run() //当前任务执行时候，对应的时间 System.out.println(延迟了+(System.currentTimeMillis() - cur) + ms); try Thread.sleep(3000); catch (InterruptedException e) e.printStackTrace(); ,3000,2000,TimeUnit.MILLISECONDS); 相比于 schedule() 方法，多了一个参数 period，就是可以指定周期执行。如上代码，周期为 2s。但其中的任务运行时间是 3s。结果是： C:\\Program Files\\Java\\jdk1.8.0_101\\bin延迟了3016ms延迟了6020ms延迟了9036ms延迟了12040ms延迟了15043ms延迟了18045msProcess finished with exit code -1 首先第一次延时 3016ms 是因为指定的 initdelay 为 3s。但后面的任务运行并不是按照 period 的周期执行，而是任务的运行时间执行。也就是说设定的周期时间不足以完成线程任务，但 scheduleFixedRate 达到设定的延迟时间了就要执行下一次。（可以从字面意思 FixedRate 固定频率理解到）。 3.3scheduleWithFixedDelay() 使用//scheduleWithFixedDelay()用法//也是加了周期运行的条件，但是必须是等到上一个任务结束后，进行周期循环。周期时间就是任务运行时间+delaystatic void scheduleWithFixedDelay() ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(1); //获取当前时间 long cur = System.currentTimeMillis(); scheduledExecutorService.scheduleWithFixedDelay(new Runnable() @Override public void run() //当前任务执行时候，对应的时间 System.out.println(延迟了+(System.currentTimeMillis() - cur) + ms); try Thread.sleep(2000); catch (InterruptedException e) e.printStackTrace(); ,3000,2000,TimeUnit.MILLISECONDS); 相比于 scheduleFixedRate()，scheduleWithFixedDelay() 是每次都要把任务执行完成后再延迟固定时间后再执行下一次。结果如下：周期为任务运行时间 + delay（指定周期时间） C:\\Program Files\\Java\\延迟了3015ms延迟了7038ms延迟了11060ms延迟了15078ms延迟了19101msProcess finished with exit code -1 四、实现原理ScheduledThreadPoolExecutor 类继承自 ThreadPoolExecutor，除了拥有普通线程池的功能之外，因为实现了 ScheduledExecutorService 接口，因而同时拥有定时器的功能。首先构造方法全部是调用了父类 ThreadPoolExecutor 的。 实例化后执行的 schedule() 方法调用了定时器，将传入的 Runnable 对象封装成 ScheduledFutureTask 对象，ScheduledFutureTask 类实现了 RunnableScheduledFuture 接口。ScheduledFutureTask 类就包含了 time 表示任务执行时间，period 表示任务执行周期等。 本地缓存的一种实现方式//缓存实体类@Datapublic class CacheEntity private String key; private Object value; private Long expireTime; public class CacheUtil private static final Logger logger = LoggerFactory.getLogger(CacheUtil.class); // 缓存数据 private final static MapString, CacheEntity CACHE_MAP = new ConcurrentHashMap(); // 定时器线程池，用于清理过期缓存 private static ScheduledExecutorService executorService = Executors.newSingleThreadScheduledExecutor(); static // 注册一个定时任务，服务启动 1000 毫秒后，每隔 500 毫秒执行一次 Runnable task = CacheUtil::clear; executorService.scheduleAtFixedRate(task, 1000L, 500L, TimeUnit.MILLISECONDS); // 添加缓存 public static void put(String key, Object value) put(key, value, 0L); // 添加缓存 public static void put(String key, Object value, Long expire) CacheEntity cacheEntity = new CacheEntity(); cacheEntity.setKey(key); cacheEntity.setValue(value); if (expire 0) // 计算过期时间 Long expireTime = System.currentTimeMillis() + Duration.ofSeconds(expire).toMillis(); cacheEntity.setExpireTime(expireTime); CACHE_MAP.put(key, cacheEntity); // 获取 public static Object get(String key) if (CACHE_MAP.containsKey(key)) return CACHE_MAP.get(key); return null; // 删除 public static void remove(String key) CACHE_MAP.remove(key); // 清除过期缓存 public static void clear() if (CACHE_MAP.isEmpty()) return; CACHE_MAP.entrySet().removeIf(entityEntry - entityEntry.getValue().getExpireTime() != null entityEntry.getValue().getExpireTime() System.currentTimeMillis());","tags":["Java基础","实际开发"],"categories":["实际开发","Java基础"]},{"title":"日志打印规范-continue","path":"/Java开发规范/日志打印规范-continue/","content":"日志打印规范一、 如何正确的打印异常的堆栈信息 一般在 catch 到异常的时候，不要使用 e.printStackTrace() 来打印异常信息。我们使用日志框架来打印信息，一般来说，日志框架的 log 级别从低到高是：debug, info, warn, error, fatal。 对于异常，一般使用 log.error() 来打印堆栈信息。下边的三个 log 语句都打印了异常，但是写法却不一样，打印出来的效果也是不同的： log.error(ERROR, Error found: , e);log.error(ERROR, Error found: + e.getMessage());log.error(ERROR, Error found: + e); 注意：千万别用 e.getMessage() 打印错误信息，因为 e.getMessage() 的打印是属于不合理的，message 给的信息有的情况并不理想 例子： public void test() try System.out.println(1/0); catch (final Exception e) log.error(ERROR, Error found: , e); log.error(ERROR, Error found: + e.getMessage()); log.error(ERROR, Error found: + e); 控制台输出： 15:41:02.124 [main] ERROR cn.cgshao.log.testLombokLog - ERRORjava.lang.ArithmeticException: / by zero\tat cn.cgshao.log.testLombokLog.test(testLombokLog.java:24)\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\tat java.lang.reflect.Method.invoke(Method.java:498)\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\tat org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)\tat org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)\tat org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\tat org.junit.runners.ParentRunner.run(ParentRunner.java:413)\tat org.junit.runner.JUnitCore.run(JUnitCore.java:137)\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)\tat com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)\tat com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)\tat com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)\tat com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)\tat com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)15:41:02.132 [main] ERROR cn.cgshao.log.testLombokLog - ERROR15:41:02.132 [main] ERROR cn.cgshao.log.testLombokLog - ERROR 1、对于第一个 log 语句，可以看到堆栈信息被打印了出来。 2、对于第二个 log 语句，只是打印出了异常的具体信息，既没有异常类名，也没有堆栈信息。 3、对于第三个 log 语句，打印出了异常的类名和具体信息，但是没有打印出来堆栈信息。 二、log.info 和 log.error 的区别 log.info 是打印的普通的日志，一般以流程中转为主 log.error 是打印异常信息的， 用 error 要打印完整异常栈， 这样的话， 追踪错误比较方便","categories":["Java开发规范"]},{"title":"git","path":"/git/git/","content":"Git操作一 、git stash其将当前未提交的修改 (即工作区和暂存区的修改) 先暂时储藏起来。 然后我们可以通过git stash list来查看储藏记录。之后通过git stash pop命令将之前最近一次储藏的修改取出来，继续之前的工作，并同时将该储藏从储藏记录列表中删除 1、单次储藏及应用我们在当前分支，添加 test.md 文件。 添加 test.md 到储藏区，使之能被 git 进行追踪 git add test.md 查看文件状态 git status输出：On branch masterYour branch is up to date with origin/master. Changes to be committed: (use git restore --staged file... to unstage) new file: test.md 进行储藏 git stash输出：Saved working directory and index state WIP on master: 709627d Initial commit 查看储藏 git stash list输出：stash@0: WIP on master: 709627d Initial commit 应用最近一次储藏 git stash pop输出：On branch masterYour branch is up to date with origin/master.Changes to be committed: (use git restore --staged file... to unstage) new file: test.mdDropped refs/stash@0 (05edd0c153191fe70ef42b9e4fdf70622d0e6831) 2、多次储藏及应用修改文件，进行储藏，重复这一步骤，再通过git stash list来查看这两次记录 git stash list输出：stash@0: WIP on master: 709627d Initial commitstash@1: WIP on master: 709627d Initial commit 从上图结果中，我们发现两次储藏记录的标识信息完全一致，只有其前面的index有别。 git 默认按如下规则标识储藏记录 (WIP 意为 work in progess, index用于后面取出所对应储藏的修改)。 stash@index: WIP on [分支名]: [最近一次的commitID] [最近一次的提交信息] 由于我们在功能分支下的两次储藏中均未发生提交，所以其提交 ID 是一致的。 这样明显会带来问题，我们在多个储藏之间，无法明确需要应用哪个，需要标识下储藏记录。 可以通过命令git stash -m [stashMessage]来标记此次储藏，以便后期查看。 通过查看储藏列表的index的可以取出指定储藏中的修改到工作区 取出指定index的储藏的修改到工作区中 git stash apply index 将指定index的储藏从储藏记录列表中删除 git stash drop index 储藏记录多的话，一个个删除太麻烦，可以进行 批量删除git stash clear 修改文件，进行储藏 git stash -m change1 输出：Saved working directory and index state On master: change1 再次修改文件，进行储藏 git stash -m change2 输出：Saved working directory and index state On master: change2 查看储藏 git stash list输出：stash@0: On master: change2stash@1: On master: change1 应用储藏，提交信息为 “change1” git stash apply 1输出：On branch masterYour branch is up to date with origin/master.Changes not staged for commit: (use git add file... to update what will be committed) (use git restore file... to discard changes in working directory) modified: README.mdno changes added to commit (use git add and/or git commit -a) 查看储藏 git stash list输出：stash@0: On master: change2stash@1: On master: change1 删除刚才应用的储藏 git stash drop 1输出：Dropped refs/stash@1 (9a6ae2acf53198fce4125161d28045c484f1b20b) 查看储藏 git stash list输出：stash@0: On master: change2 再添加储藏 git stash -m add新change 输出：Saved working directory and index state On master: add新change 查看储藏 git stash list输出：stash@0: On master: add新changestash@1: On master: change2 删除所有储藏 git stash clear 查看储藏 git stash list 输出为空，代表已经完全清除 当删除index为 0 项，index为 1 项中的index会变为 0。即index会动态从 0 开始排序的，可看如下示例。 git stash list输出：stash@0: On master: change2stash@1: On master: change1git stash drop 0 输出：Dropped refs/stash@0 (fb3d70dea8b439cd09cce774cf622638a341af9c)git stash list输出：stash@0: On master: change1 3、对特定范围文件进行储藏默认情况下，只要在 git 追踪范围的文件，均可以进行储藏。我们可以进一步区分特定范围的文件进行储藏。 git stash [-u|--include-untracked]：对未追踪文件也进行储藏 git stash [-S|--staged]: 只对暂存区文件进行储藏 git stash [-a|--all]: 对所有文件进行储藏 提前准备好三种 git 状态不同的文件。新添加的文件（即为未追踪文件），工作区修改的文件，提交到暂存区的文件 git status输出：git statusOn branch masterYour branch is up to date with origin/master. Changes to be committed: (use git restore --staged file... to unstage) modified: README.md Changes not staged for commit: (use git add file... to update what will be committed) (use git restore file... to discard changes in working directory) modified: README.en.mdUntracked files: (use git add file... to include in what will be committed) test.md 对未追踪文件也进行储藏 git stash -u输出：Saved working directory and index state WIP on master: 709627d Initial commit 查看文件状态 git status输出：On branch masterYour branch is up to date with origin/master.nothing to commit, working tree clean 对刚才储藏进行应用 git stash pop输出：On branch masterYour branch is up to date with origin/master.Changes not staged for commit: (use git add file... to update what will be committed) (use git restore file... to discard changes in working directory) modified: README.en.md modified: README.md Untracked files: (use git add file... to include in what will be committed) test.mdno changes added to commit (use git add and/or git commit -a)Dropped refs/stash@0 (23b4e04af9cc34612f458648e838f552a1f99169) 将 README.md 添加到暂存区 git add README.md 查看文件状态 git status输出：On branch masterYour branch is up to date with origin/master. Changes to be committed: (use git restore --staged file... to unstage) modified: README.md Changes not staged for commit: (use git add file... to update what will be committed) (use git restore file... to discard changes in working directory) modified: README.en.mdUntracked files: (use git add file... to include in what will be committed) test.md 只对暂存区文件进行储藏 git stash -S输出：Saved working directory and index state WIP on master: 709627d Initial commit 查看文件状态 git status输出：On branch masterYour branch is up to date with origin/master.Changes not staged for commit: (use git add file... to update what will be committed) (use git restore file... to discard changes in working directory) modified: README.en.mdUntracked files: (use git add file... to include in what will be committed) test.mdno changes added to commit (use git add and/or git commit -a) 重复之前步骤。将储藏进行应用，同时将 README.md 添加到暂存区 全部文件添加到储藏区 git stash -a输出：Saved working directory and index state WIP on master: 709627d Initial commit 查看文件状态 git status输出：On branch masterYour branch is up to date with origin/master.nothing to commit, working tree clean 若想对stash命令了解更多，可以通过命令git stash --help查阅。 二、 git commit —amend修改最新一条commit注释git commit --amend 三、 git rebase git rebase 会把你当前分支的commit 放到公共分支的最后面, 叫变基 rebase 可以合并commit、 拆分commit 、删除commit是一条强大又危险的命令 如果只是 合并分支,建议使用git merge命令 不建议使用git rebase命令 git rebase详解 git rebase -i 交互式rebase通过该命令可以合并commit、 拆分commit 、删除commit是一条强大又危险的命令. 四、git checkout -b [feature] [origin/feature]# 创建分支git branch branch# 切换到某分支git checkout branch# 创建并切换到新分支git checkout -b branch# 基于远程分支创建分支git checkout -b [feature] [origin/feature] 五、git remote命令git remote 命令用来创建、查看和删除本地仓库 与 其他仓库之间的连接 remote链接更像是一种书签标记而不是与其他仓库之间的硬连接。这种标记通过一种简单的命名来代替不便使用的完整URL，而不是提供一种与仓库之间的实时通道。# 列出所有远程主机git remote# 显示所有远程仓库git remote -v# 限制某个远程仓库的详细信息git remote show [origin] # 关联远程分支 (相当于修改./.git/config文件)git remote add name url# 删除别名为name的远端仓库的关联关系git remote rm name# 将别名为old-name的远端仓库的关联关系重命名为new-name。git remote rename old-name new-name 六、git push 命令# 标准命令git push 远程主机名 本地分支名:远程分支名# 如果本地分支名与远程分支名相同，则可以省略冒号git push 远程主机名 本地分支名#以下命令将本地的 master 分支推送到 origin 主机的 master 分支。git push origin master# 相当于git push origin master:master# 第一次提交 git push -u origin master (-u 相当于 --set-upstream)# 强制推送git push --force origin master","tags":["git"],"categories":["git"]},{"title":"CompletableFuture常用方法","path":"/Java基础/CompletableFuture常用方法/","content":"CompletableFuture 常用方法1. 创建异步任务CompletableFuture.runAsync(): 创建一个没有返回值的异步任务。CompletableFutureVoid runComplete = CompletableFuture.runAsync(()- System.out.println(没有返回值的异步任务)); CompletableFuture.supplyAsync(): 常见一个有返回值的异步任务。CompletableFutureString supportComplete = CompletableFuture.supplyAsync(()- 有返回值的异步任务); 可以传入自定义的线程池执行任务: ExecutorService executor = Executors.newCachedThreadPool(); CompletableFutureVoid runComplete = CompletableFuture.runAsync(()- System.out.println(没有返回值的异步任务),executor); CompletableFutureString supportComplete = CompletableFuture.supplyAsync(()- 有返回值的异步任务,executor); 2. 异步任务完成之后的回调1️⃣ thenRun/thenRunAsyncthenRun 通俗点讲就是，做完一个任务后，再做第二个任务。某个任务执行完成后，执行回调方法，但是前后两个任务没有参数传递，第二个任务也没有返回值。 CompletableFutureString supportComplete = CompletableFuture.supplyAsync(()- System.out.println(有返回值的异步任务); return 有返回值的异步任务; ); CompletableFutureVoid thenRun = supportComplete.thenRun(() - System.out.println(完成任务之后的回调，没有参数也没有返回值)); System.out.println(thenRun.join()); thenRun/thenRunAsync 的区别： 如果你执行第一个任务的时候，传入了一个自定义线程池 调用 thenRun 方法执行第二个任务时，则第二个任务和第一个任务是共用同一个线程池。 调用 thenRunAsync 执行第二个任务时，则第一个任务使用的是你自己传入的线程池，第二个任务使用的是 ForkJoin 线程池。 2️⃣ thenAccept/thenAcceptAsyncthenAccept 和 thenAcceptAsync 指的是做完第一个任务之后，将第一个任务的返回值作为参数传到 thenAccept 方法中，thenAccept 和 thenAcceptAsync 没有返回值。 CompletableFutureString supportComplete = CompletableFuture.supplyAsync(()- System.out.println(有返回值的异步任务); return 有返回值的异步任务;); CompletableFutureVoid thenRun = supportComplete.thenAccept((s) - System.out.println(完成任务之后的回调，有参数，参数为 + s + 没有返回值)); System.out.println(thenRun.join()); 两者的区别依然是共用线程池或者是第二个任务用 ForkJoin 线程池的区别。 3️⃣ thenApply 和 thenApplyAsyncthenApply 和 thenApplyAsync 指的是第一个任务执行完成后，执行第二个回调方法任务，会将该任务的执行结果，作为入参，传递到回调方法中，并且回调方法是有返回值的。 CompletableFutureString supportComplete = CompletableFuture.supplyAsync(()- System.out.println(有返回值的异步任务); return 有返回值的异步任务; );CompletableFutureString thenRun = supportComplete.thenApplyAsync((s) - System.out.println(完成任务之后的回调，有参数，参数为 + s + 有返回值); return 完成任务之后的回调，有参数，参数为 + s + 有返回值; ); System.out.println(thenRun.join()); thenApply 和 thenApplyAsync 的区别和上面两者一样。 4️⃣ exceptionallyexceptionally 方法表示，某个任务执行异常时，执行的回调方法; 并且有抛出异常作为参数，传递到回调方法。参数为异常，有返回值。 CompletableFutureString supportComplete = CompletableFuture.supplyAsync(()- System.out.println(有返回值的异步任务); throw new RuntimeException(); ); CompletableFutureString exceptionFuture = supportComplete.exceptionally((e) - e.printStackTrace(); return 程序出现异常; ); System.out.println(exceptionFuture.get()); 5️⃣ whenCompletewhenComplete 方法表示，某个任务执行完成后，执行的回调方法，无返回值；并且 whenComplete 方法的参数是上个任务的结果。 CompletableFutureString supportComplete = CompletableFuture.supplyAsync(()- System.out.println(有返回值的异步任务); return 有返回值的异步任务; ); CompletableFutureString whenComplete = supportComplete.whenComplete((e,throwable) - System.out.println(e); ); System.out.println(whenComplete.get()); whenComplete 接收的参数为两个，第一个是上个任务的结果，另一个是异常。因此即使主任务有异常，依然会执行。 6️⃣：handlehandle 和 whenComplete 差不多，都是表示某个任务执行完成后，执行的回调方法，handle 方法的参数时上个任务的结果，但是 handle 是有返回值的，whenComplete 没有返回值。 CompletableFutureString supportComplete = CompletableFuture.supplyAsync(()- System.out.println(当前线程名称为: + Thread.currentThread().getName()); System.out.println(有返回值的异步任务); return 有返回值的异步任务; );CompletableFutureString handle = supportComplete.handle((e,throwable) - System.out.println(当前线程名称为: + Thread.currentThread().getName()); System.out.println(e); return 有返回值; ); System.out.println(handle.get()); 3. 多任务组合处理：1️⃣：AND 组合关系：thenCombine / thenAcceptBoth / runAfterBoth三者都表示：将两个 CompletableFuture 组合起来，只有这两个都正常执行完了，才会执行某个任务。 区别在于： thenCombine：会将两个任务的执行结果作为方法入参，传递到指定方法中，且有返回值 thenAcceptBoth: 会将两个任务的执行结果作为方法入参，传递到指定方法中，且无返回值 runAfterBoth 不会把执行结果当做方法入参，且没有返回值。 thenCombine/ thenCombineAsync实例CompletableFutureString completableFuture = CompletableFuture.supplyAsync(()-第一个异步任务);ExecutorService executorService = Executors.newFixedThreadPool(10); CompletableFutureString supportComplete = CompletableFuture.supplyAsync(() - 第二个异步任务,executorService) .thenCombineAsync(completableFuture,(s, w) - System.out.println(w); System.out.println(s); return 两个异步任务的组合; ,executorService); System.out.println(supportComplete.get()); executorService.shutdown(); 2️⃣ OR 组合的关系：applyToEither / acceptEither / runAfterEither三者都表示：将两个 CompletableFuture 组合起来，只要其中一个执行完了, 就会执行某个任务。感觉业务应用场景不会很多 区别在于： applyToEither：会将已经执行完成的任务，作为方法入参，传递到指定方法中，且有返回值 acceptEither: 会将已经执行完成的任务，作为方法入参，传递到指定方法中，且无返回值 runAfterEither：不会把执行结果当做方法入参，且没有返回值。 CompletableFutureString completableFuture = CompletableFuture.supplyAsync(()- try Thread.sleep(2000); catch (InterruptedException e) e.printStackTrace(); return 第一个异步任务; ); ExecutorService executorService = Executors.newFixedThreadPool(10); CompletableFuture.supplyAsync(() - 第二个异步任务,executorService) .acceptEitherAsync(completableFuture, System.out::println,executorService); executorService.shutdown(); 3️⃣ AllOf所有任务都执行完成后，才执行 allOf 返回的 CompletableFuture。如果任意一个任务异常，allOf 的 CompletableFuture，执行 get 方法，会抛出异常。 CompletableFutureVoid completableFuture = CompletableFuture.runAsync(()- System.out.println(第一个异步任务执行完了); );CompletableFutureVoid runAsync = CompletableFuture.runAsync(() - System.out.println(第二个异步任务执行完了); ); CompletableFuture.allOf(completableFuture,runAsync).whenComplete((a,b) - System.out.println(finish);); 4️⃣ AnyOf任意一个任务执行完，就执行 anyOf 返回的 CompletableFuture。如果执行的任务异常，anyOf 的 CompletableFuture，执行 get 方法，会抛出异常。 CompletableFutureVoid completableFuture = CompletableFuture.runAsync(()- try Thread.sleep(2000); catch (InterruptedException e) e.printStackTrace(); System.out.println(第一个异步任务执行完了); ); CompletableFutureVoid runAsync = CompletableFuture.runAsync(() - System.out.println(第二个异步任务执行完了); ); CompletableFuture.anyOf(completableFuture,runAsync).whenComplete((a,b) - System.out.println(finish); ); 5️⃣ thenComposethenCompose 方法会在某个任务执行完成后，将该任务的执行结果, 作为方法入参, 去执行指定的方法。该方法会返回一个新的 CompletableFuture 实例 如果该 CompletableFuture 实例的 result 不为 null，则返回一个基于该 result 新的 CompletableFuture 实例； 如果该 CompletableFuture 实例为 null，然后就执行这个新任务 CompletableFutureString completableFuture = CompletableFuture.completedFuture(第一个任务);CompletableFutureString stringCompletableFuture = CompletableFuture.supplyAsync(() - 第二个任务) .thenCompose(data - System.out.println(data); return completableFuture; ); System.out.println(stringCompletableFuture.get()); 4. 注意事项1️⃣ Future 需要获取返回值，才能获取异常信息Future 需要获取返回值，才能获取到异常信息。如果不加 get()/join() 方法，看不到异常信息。 2️⃣CompletableFuture 的 get() 方法是阻塞的。如果使用它来获取异步调用的返回值，需要添加超时时间~ // 反例 CompletableFuture.get(); // 正例 CompletableFuture.get(5, TimeUnit.SECONDS); 3️⃣默认线程池的注意点CompletableFuture 代码中又使用了默认的线程池，处理的线程个数是电脑 CPU 核数 - 1。在大量请求过来的时候，处理逻辑复杂的话，响应会很慢。一般建议使用自定义线程池，优化线程池配置参数。 4️⃣自定义线程池时，注意饱和策略CompletableFuture 的 get() 方法是阻塞的，我们一般建议使用 future.get(3, TimeUnit.SECONDS)。并且一般建议使用自定义线程池。 但是如果线程池拒绝策略是 DiscardPolicy 或者 DiscardOldestPolicy，当线程池饱和时，会直接丢弃任务，不会抛弃异常。因此建议，CompletableFuture 线程池策略最好使用 AbortPolicy，然后耗时的异步线程，做好线程池隔离哈。","tags":["Java基础","CompletableFuture"],"categories":["Java基础"]},{"title":"CompletableFuture入门","path":"/Java基础/CompletableFuture入门/","content":"Java 8 CompletableFuture 教程 原文链接：https://www.callicoder.com/java-8-completablefuture-tutorial/ 什么是 CompletableFuture？在 Java 中 CompletableFuture 用于异步编程，异步编程是编写非阻塞的代码，运行的任务在一个单独的线程，与主线程隔离，并且会通知主线程它的进度，成功或者失败。 在这种方式中，主线程不会被阻塞，不需要一直等到子线程完成。主线程可以并行的执行其他任务。 使用这种并行方式，可以极大的提高程序的性能。 Future vs CompletableFutureCompletableFuture 是 Future API 的扩展。 Future 被用于作为一个异步计算结果的引用。提供一个 isDone() 方法来检查计算任务是否完成。当任务完成时，get() 方法用来接收计算任务的结果。 从 Callbale 和 Future 教程可以学习更多关于 Future 知识. Future API 是非常好的 Java 异步编程进阶，但是它缺乏一些非常重要和有用的特性。 Future 的局限性 不能手动完成 当你写了一个函数，用于通过一个远程 API 获取一个电子商务产品最新价格。因为这个 API 太耗时，你把它允许在一个独立的线程中，并且从你的函数中返回一个 Future。现在假设这个 API 服务宕机了，这时你想通过该产品的最新缓存价格手工完成这个 Future 。你会发现无法这样做。 Future 的结果在非阻塞的情况下，不能执行更进一步的操作 Future 不会通知你它已经完成了，它提供了一个阻塞的 get() 方法通知你结果。你无法给 Future 植入一个回调函数，当 Future 结果可用的时候，用该回调函数自动的调用 Future 的结果。 多个 Future 不能串联在一起组成链式调用 有时候你需要执行一个长时间运行的计算任务，并且当计算任务完成的时候，你需要把它的计算结果发送给另外一个长时间运行的计算任务等等。你会发现你无法使用 Future 创建这样的一个工作流。 不能组合多个 Future 的结果 假设你有 10 个不同的 Future，你想并行的运行，然后在它们运行未完成后运行一些函数。你会发现你也无法使用 Future 这样做。 没有异常处理 Future API 没有任务的异常处理结构居然有如此多的限制，幸好我们有 CompletableFuture，你可以使用 CompletableFuture 达到以上所有目的。 CompletableFuture 实现了 Future 和 CompletionStage接口，并且提供了许多关于创建，链式调用和组合多个 Future 的便利方法集，而且有广泛的异常处理支持。 创建 CompletableFuture1. 简单的例子 可以使用如下无参构造函数简单的创建 CompletableFuture： CompletableFutureString completableFuture = new CompletableFuture(); 这是一个最简单的 CompletableFuture，想获取 CompletableFuture 的结果可以使用 CompletableFuture.get() 方法： String result = completableFuture.get() get() 方法会一直阻塞直到 Future 完成。因此，以上的调用将被永远阻塞，因为该 Future 一直不会完成。 你可以使用 CompletableFuture.complete() 手工的完成一个 Future： completableFuture.complete(Futures Result) 所有等待这个 Future 的客户端都将得到一个指定的结果，并且 completableFuture.complete() 之后的调用将被忽略。 2. 使用 runAsync() 运行异步计算 如果你想异步的运行一个后台任务并且不想改任务返回任务东西，这时候可以使用 CompletableFuture.runAsync()方法，它持有一个 Runnable 对象，并返回 CompletableFutureVoid。 // Run a task specified by a Runnable Object asynchronously.CompletableFutureVoid future = CompletableFuture.runAsync(new Runnable() @Override public void run() // Simulate a long-running Job try TimeUnit.SECONDS.sleep(1); catch (InterruptedException e) throw new IllegalStateException(e); System.out.println(Ill run in a separate thread than the main thread.); );// Block and wait for the future to completefuture.get() 你也可以以 lambda 表达式的形式传入 Runnable 对象 3. 使用 supplyAsync() 运行一个异步任务并且返回结果 当任务不需要返回任何东西的时候， CompletableFuture.runAsync() 非常有用。但是如果你的后台任务需要返回一些结果应该要怎么样？ CompletableFuture.supplyAsync() 就是你的选择。它持有supplierT 并且返回CompletableFutureT，T 是通过调用 传入的 supplier 取得的值的类型。 // Run a task specified by a Supplier object asynchronouslyCompletableFutureString future = CompletableFuture.supplyAsync(new SupplierString() @Override public String get() try TimeUnit.SECONDS.sleep(1); catch (InterruptedException e) throw new IllegalStateException(e); return Result of the asynchronous computation; );// Block and get the result of the FutureString result = future.get();System.out.println(result); SupplierT 是一个简单的函数式接口，表示 supplier 的结果。它有一个get()方法，该方法可以写入你的后台任务中，并且返回结果。 你可以使用 lambda 表达式使得上面的示例更加简明： // Using Lambda ExpressionCompletableFutureString future = CompletableFuture.supplyAsync(() - try TimeUnit.SECONDS.sleep(1); catch (InterruptedException e) throw new IllegalStateException(e); return Result of the asynchronous computation;); 一个关于 Executor 和 Thread Pool 笔记 你可能想知道，我们知道runAsync()和supplyAsync()方法在单独的线程中执行他们的任务。但是我们不会永远只创建一个线程。 CompletableFuture 可以从全局的 ForkJoinPool.commonPool() 获得一个线程中执行这些任务。 但是你也可以创建一个线程池并传给runAsync()和supplyAsync()方法来让他们从线程池中获取一个线程执行它们的任务。 CompletableFuture API 的所有方法都有两个变体 - 一个接受Executor作为参数，另一个不这样： // Variations of runAsync() and supplyAsync() methodsstatic CompletableFutureVoid runAsync(Runnable runnable)static CompletableFutureVoid runAsync(Runnable runnable, Executor executor)static U CompletableFutureU supplyAsync(SupplierU supplier)static U CompletableFutureU supplyAsync(SupplierU supplier, Executor executor) 创建一个线程池，并传递给其中一个方法： Executor executor = Executors.newFixedThreadPool(10);CompletableFutureString future = CompletableFuture.supplyAsync(() - try TimeUnit.SECONDS.sleep(1); catch (InterruptedException e) throw new IllegalStateException(e); return Result of the asynchronous computation;, executor); 在 CompletableFuture 转换和运行CompletableFuture.get()方法是阻塞的。它会一直等到 Future 完成并且在完成后返回结果。 但是，这是我们想要的吗？对于构建异步系统，我们应该附上一个回调给 CompletableFuture，当 Future 完成的时候，自动的获取结果。 如果我们不想等待结果返回，我们可以把需要等待 Future 完成执行的逻辑写入到回调函数中。 可以使用 thenApply(), thenAccept() 和thenRun()方法附上一个回调给 CompletableFuture。 1. thenApply() 可以使用 thenApply() 处理和改变 CompletableFuture 的结果。持有一个FunctionR,T作为参数。FunctionR,T是一个简单的函数式接口，接受一个 T 类型的参数，产出一个 R 类型的结果。 // Create a CompletableFutureCompletableFutureString whatsYourNameFuture = CompletableFuture.supplyAsync(() - try TimeUnit.SECONDS.sleep(1); catch (InterruptedException e) throw new IllegalStateException(e); return Rajeev;);// Attach a callback to the Future using thenApply()CompletableFutureString greetingFuture = whatsYourNameFuture.thenApply(name - return Hello + name;);// Block and get the result of the future.System.out.println(greetingFuture.get()); // Hello Rajeev 你也可以通过附加一系列的thenApply()在回调方法 在 CompletableFuture 写一个连续的转换。这样的话，结果中的一个 thenApply方法就会传递给该系列的另外一个 thenApply方法。 CompletableFutureString welcomeText = CompletableFuture.supplyAsync(() - try TimeUnit.SECONDS.sleep(1); catch (InterruptedException e) throw new IllegalStateException(e); return Rajeev;).thenApply(name - return Hello + name;).thenApply(greeting - return greeting + , Welcome to the CalliCoder Blog;);System.out.println(welcomeText.get());// Prints - Hello Rajeev, Welcome to the CalliCoder Blog 2. thenAccept() 和 thenRun() 如果你不想从你的回调函数中返回任何东西，仅仅想在 Future 完成后运行一些代码片段，你可以使用thenAccept()和 thenRun()方法，这些方法经常在调用链的最末端的最后一个回调函数中使用。 CompletableFuture.thenAccept()持有一个ConsumerT，返回一个CompletableFutureVoid。它可以访问CompletableFuture的结果： // thenAccept() exampleCompletableFuture.supplyAsync(() - return ProductService.getProductDetail(productId);).thenAccept(product - System.out.println(Got product detail from remote service + product.getName())); 虽然thenAccept()可以访问 CompletableFuture 的结果，但thenRun()不能访 Future 的结果，它持有一个 Runnable 返回 CompletableFuture： // thenRun() exampleCompletableFuture.supplyAsync(() - // Run some computation ).thenRun(() - // Computation Finished.); 异步回调方法的笔记 CompletableFuture 提供的所有回调方法都有两个变体： // thenApply() variants U CompletableFutureU thenApply(Function? super T,? extends U fn) U CompletableFutureU thenApplyAsync(Function? super T,? extends U fn) U CompletableFutureU thenApplyAsync(Function? super T,? extends U fn, Executor executor) 这些异步回调变体通过在独立的线程中执行回调任务帮助你进一步执行并行计算。 以下示例： CompletableFuture.supplyAsync(() - try TimeUnit.SECONDS.sleep(1); catch (InterruptedException e) throw new IllegalStateException(e); return Some Result).thenApply(result - /* Executed in the same thread where the supplyAsync() task is executed or in the main thread If the supplyAsync() task completes immediately (Remove sleep() call to verify) */ return Processed Result) 在以上示例中，在thenApply()中的任务和在supplyAsync()中的任务执行在相同的线程中。任何supplyAsync()立即执行完成, 那就是执行在主线程中（尝试删除 sleep 测试下）。 为了控制执行回调任务的线程，你可以使用异步回调。如果你使用thenApplyAsync()回调，将从ForkJoinPool.commonPool()获取不同的线程执行。 CompletableFuture.supplyAsync(() - return Some Result).thenApplyAsync(result - // Executed in a different thread from ForkJoinPool.commonPool() return Processed Result) 此外，如果你传入一个Executor到thenApplyAsync()回调中，，任务将从 Executor 线程池获取一个线程执行。 Executor executor = Executors.newFixedThreadPool(2);CompletableFuture.supplyAsync(() - return Some result).thenApplyAsync(result - // Executed in a thread obtained from the executor return Processed Result, executor); 组合两个 CompletableFuture1. 使用 thenCompose()组合两个独立的 future 假设你想从一个远程 API 中获取一个用户的详细信息，一旦用户信息可用，你想从另外一个服务中获取他的贷方。 考虑下以下两个方法getUserDetail()和getCreditRating()的实现： CompletableFutureUser getUsersDetail(String userId) return CompletableFuture.supplyAsync(() - UserService.getUserDetails(userId);\t);\tCompletableFutureDouble getCreditRating(User user) return CompletableFuture.supplyAsync(() - CreditRatingService.getCreditRating(user);\t); 现在让我们弄明白当使用了thenApply()后是否会达到我们期望的结果 - CompletableFutureCompletableFutureDouble result = getUserDetail(userId).thenApply(user - getCreditRating(user)); 在此示例中，Supplier函数传入thenApply将返回一个简单的值，但是在本例中，将返回一个 CompletableFuture。以上示例的最终结果是一个嵌套的 CompletableFuture。 如果你想获取最终的结果给最顶层 future，使用 thenCompose()方法代替 - CompletableFutureDouble result = getUserDetail(userId).thenCompose(user - getCreditRating(user)); 因此，规则就是 - 如果你的回调函数返回一个 CompletableFuture，但是你想从 CompletableFuture 链中获取一个直接合并后的结果，这时候你可以使用thenCompose()。 2. 使用thenCombine()组合两个独立的 future ，thenCompose()被用于当一个 future 依赖另外一个 future 的时候用来组合两个 future。thenCombine()被用来当两个独立的Future都完成的时候，用来做一些事情。 System.out.println(Retrieving weight.);CompletableFutureDouble weightInKgFuture = CompletableFuture.supplyAsync(() - try TimeUnit.SECONDS.sleep(1); catch (InterruptedException e) throw new IllegalStateException(e); return 65.0;);System.out.println(Retrieving height.);CompletableFutureDouble heightInCmFuture = CompletableFuture.supplyAsync(() - try TimeUnit.SECONDS.sleep(1); catch (InterruptedException e) throw new IllegalStateException(e); return 177.8;);System.out.println(Calculating BMI.);CompletableFutureDouble combinedFuture = weightInKgFuture .thenCombine(heightInCmFuture, (weightInKg, heightInCm) - Double heightInMeter = heightInCm/100; return weightInKg/(heightInMeter*heightInMeter););System.out.println(Your BMI is - + combinedFuture.get()); 当两个 Future 都完成的时候，传给 ``thenCombine() 的回调函数将被调用。 组合多个 CompletableFuture我们使用thenCompose()和 thenCombine()把两个 CompletableFuture 组合在一起。现在如果你想组合任意数量的 CompletableFuture，应该怎么做？我们可以使用以下两个方法组合任意数量的 CompletableFuture。 static CompletableFutureVoid allOf(CompletableFuture?... cfs)static CompletableFutureObject anyOf(CompletableFuture?... cfs) 1. CompletableFuture.allOf() CompletableFuture.allOf的使用场景是当你一个列表的独立 future，并且你想在它们都完成后并行的做一些事情。 假设你想下载一个网站的 100 个不同的页面。你可以串行的做这个操作，但是这非常消耗时间。因此你想写一个函数，传入一个页面链接，返回一个 CompletableFuture，异步的下载页面内容。 CompletableFutureString downloadWebPage(String pageLink) return CompletableFuture.supplyAsync(() - // Code to download and return the web pages content\t); 现在，当所有的页面已经下载完毕，你想计算包含关键字CompletableFuture页面的数量。可以使用CompletableFuture.allOf()达成目的。 ListString webPageLinks = Arrays.asList(...)\t// A list of 100 web page links// Download contents of all the web pages asynchronouslyListCompletableFutureString pageContentFutures = webPageLinks.stream() .map(webPageLink - downloadWebPage(webPageLink)) .collect(Collectors.toList());// Create a combined Future using allOf()CompletableFutureVoid allFutures = CompletableFuture.allOf( pageContentFutures.toArray(new CompletableFuture[pageContentFutures.size()])); 使用CompletableFuture.allOf()的问题是它返回 CompletableFuture。但是我们可以通过写一些额外的代码来获取所有封装的 CompletableFuture 结果。 // When all the Futures are completed, call `future.join()` to get their results and collect the results in a list -CompletableFutureListString allPageContentsFuture = allFutures.thenApply(v - return pageContentFutures.stream() .map(pageContentFuture - pageContentFuture.join()) .collect(Collectors.toList());); 花一些时间理解下以上代码片段。当所有 future 完成的时候，我们调用了future.join()，因此我们不会在任何地方阻塞。 join()方法和get()方法非常类似，这唯一不同的地方是如果最顶层的 CompletableFuture 完成的时候发生了异常，它会抛出一个未经检查的异常。 现在让我们计算包含关键字页面的数量。 // Count the number of web pages having the CompletableFuture keyword.CompletableFutureLong countFuture = allPageContentsFuture.thenApply(pageContents - return pageContents.stream() .filter(pageContent - pageContent.contains(CompletableFuture)) .count(););System.out.println(Number of Web Pages having CompletableFuture keyword - + countFuture.get()); 2. CompletableFuture.anyOf() CompletableFuture.anyOf()和其名字介绍的一样，当任何一个 CompletableFuture 完成的时候【相同的结果类型】，返回一个新的 CompletableFuture。以下示例： CompletableFutureString future1 = CompletableFuture.supplyAsync(() - try TimeUnit.SECONDS.sleep(2); catch (InterruptedException e) throw new IllegalStateException(e); return Result of Future 1;);CompletableFutureString future2 = CompletableFuture.supplyAsync(() - try TimeUnit.SECONDS.sleep(1); catch (InterruptedException e) throw new IllegalStateException(e); return Result of Future 2;);CompletableFutureString future3 = CompletableFuture.supplyAsync(() - try TimeUnit.SECONDS.sleep(3); catch (InterruptedException e) throw new IllegalStateException(e); return Result of Future 3;);CompletableFutureObject anyOfFuture = CompletableFuture.anyOf(future1, future2, future3);System.out.println(anyOfFuture.get()); // Result of Future 2 在以上示例中，当三个中的任何一个 CompletableFuture 完成， anyOfFuture就会完成。因为future2的休眠时间最少，因此她最先完成，最终的结果将是future2的结果。 CompletableFuture.anyOf()传入一个 Future 可变参数，返回 CompletableFuture。CompletableFuture.anyOf()的问题是如果你的 CompletableFuture 返回的结果是不同类型的，这时候你讲会不知道你最终 CompletableFuture 是什么类型。 CompletableFuture 异常处理我们探寻了怎样创建 CompletableFuture，转换它们，并组合多个 CompletableFuture。现在让我们弄明白当发生错误的时候我们应该怎么做。 首先让我们明白在一个回调链中错误是怎么传递的。思考下以下回调链： CompletableFuture.supplyAsync(() - // Code which might throw an exception\treturn Some result;).thenApply(result - return processed result;).thenApply(result - return result after further processing;).thenAccept(result - // do something with the final result); 如果在原始的supplyAsync()任务中发生一个错误，这时候没有任何thenApply会被调用并且 future 将以一个异常结束。如果在第一个thenApply发生错误，这时候第二个和第三个将不会被调用，同样的，future 将以异常结束。 1. 使用 exceptionally() 回调处理异常 exceptionally()回调给你一个从原始 Future 中生成的错误恢复的机会。你可以在这里记录这个异常并返回一个默认值。 Integer age = -1;CompletableFutureString maturityFuture = CompletableFuture.supplyAsync(() - if(age 0) throw new IllegalArgumentException(Age can not be negative); if(age 18) return Adult; else return Child; ).exceptionally(ex - System.out.println(Oops! We have an exception - + ex.getMessage()); return Unknown!;);System.out.println(Maturity : + maturityFuture.get()); 2. 使用 handle() 方法处理异常 API 提供了一个更通用的方法 - handle()从异常恢复，无论一个异常是否发生它都会被调用。 Integer age = -1;CompletableFutureString maturityFuture = CompletableFuture.supplyAsync(() - if(age 0) throw new IllegalArgumentException(Age can not be negative); if(age 18) return Adult; else return Child; ).handle((res, ex) - if(ex != null) System.out.println(Oops! We have an exception - + ex.getMessage()); return Unknown!; return res;);System.out.println(Maturity : + maturityFuture.get()); 如果异常发生，res参数将是 null，否则，ex将是 null。","tags":["Java基础","CompletableFuture","实际开发"],"categories":["Java基础"]},{"title":"线程池-核心参数推荐","path":"/线程池/线程池-核心参数推荐/","content":"线程池-核心参数设置推荐corePoolSize = \\frac{1s需要执行的并发任务数}{1个线程1s可执行任务数} = \\frac{每台机器qps * 当前接口并发任务数}{\\frac{1000ms}{每个任务的平均耗时(ms)}} = \\frac{\\frac{qps}{服务器数量}* 当前接口并发任务数}{\\frac{1000ms}{每个任务的平均耗时(ms)}}maximumPoolSize = 2 * corePoolSizequeusSize = maximumPoolSize * \\frac{1000ms * 最多等待的秒数}{每个任务的平均耗时} 线程池参数设置标准： 线程池3个标准参数分别是corePoolSize（核心线程数）、maximumPoolSize（最大线程数）、queueSize（队列长度），关于这三个参数的作用可以参考一下引用： 首先，所有任务的调度都是由execute方法完成的，这部分完成的工作是：检查现在线程池的运行状态、运行线程数、运行策略，决定接下来执行的流程，是直接申请线程执行，或是缓冲到队列中执行，亦或是直接拒绝该任务。其执行过程如下： 首先检测线程池运行状态，如果不是RUNNING，则直接拒绝，线程池要保证在RUNNING的状态下执行任务。 如果workerCount corePoolSize，则创建并启动一个线程来执行新提交的任务。 如果workerCount = corePoolSize，且线程池内的阻塞队列未满，则将任务添加到该阻塞队列中。 如果workerCount = corePoolSize workerCount maximumPoolSize，且线程池内的阻塞队列已满，则创建并启动一个线程来执行新提交的任务。 如果workerCount = maximumPoolSize，并且线程池内的阻塞队列已满, 则根据拒绝策略来处理该任务, 默认的处理方式是直接抛异常。 ❤️在考虑如何设置这三个参数之前，首先需要明确的问题是：这3个参数不可能在代码阶段就能确定最优值，只能在上线之后根据实际情况进行调整。基于这个前提，我们需要确认的两个问题如下： 如何在代码阶段确定相对靠谱的3值 如何在上线之后进行调整 编码阶段确定核心3值： 这3个值主要跟以下几个要素有关：接口qps、服务器数量、每个接口中的异步线程数量、每个异步线程的耗时。 这里需要重点说明一下，一些比较权威的书籍或论文中都会以cpu核数、cpu利用率、cpu等待时间和cpu计算时间用于计算每台机器应该开辟多少线程，但这种方式根本没有考虑实际情况，无法进行逻辑，所以我们现在只根据业务的实际场景讨论适合我们自己的3值设置方式。 一般我们都会认为核心线程数根据平峰期的qps来计算，最大线程数根据高峰期qps来计算，但是这样计算风险太高了，所以我们可以用高峰期的qps来计算核心线程数，最大线程数设置为核心线程数的2倍这种计算方式。根据高峰期qps计算核心线程数的方式如下 corePoolSize = \\frac{1s需要执行的并发任务数}{1个线程1s可执行任务数} = \\frac{每台机器qps * 当前接口并发任务数}{\\frac{1000ms}{每个任务的平均耗时(ms)}} = \\frac{\\frac{qps}{服务器数量}* 当前接口并发任务数}{\\frac{1000ms}{每个任务的平均耗时(ms)}}比如，我们判断一个接口的qps为500，线上有10台服务器，接口共调用了5个并发任务，每个任务平均年耗时200ms，那么计算方式就是： corePoolSize = \\frac{\\frac{500}{10}*5}{\\frac{1000}{200}} = 50 maximumPoolSize = 2 * corePoolSize = 100在确定了corePoolSize和maximumPoolSize之后，如何确定队列长度呢？其实我们设想一个极端场景就可以了，比如所有maximumPoolSize线程都在满负荷工作，我们希望队列最后一个待执行任务等多久？如果我们希望它最多等1秒，那么所有线程满负荷工作1秒可以处理的线程数就应该是队列长度，如果是最多等待2秒，那就应该是所有线程2秒可以执行的任务数就是队列长度，计算方式如下： queueSize = maximumPoolSize * \\frac{1000ms * 最多等待的秒数}{每个任务的平均耗时}还是基于上述示例计算队列长度，应该为： queueSize = 100 * \\frac{1000ms*2}{200ms} = 1000综上所述，如果我们判断一个接口的qps为500，线上有10台服务器，接口共调用了5个并发任务，每个任务平均年耗时200ms，并且我们希望每个任务等待不要超过2秒，那么核心线程数应该设置为50，最大线程数设置为100，队列设置为1000 另外需要注意的是，推荐每个接口自己使用自己的线程池，不要两个接口同时使用同一个线程池，否则上述计算指标中的【当前接口并发任务数】不容易计算，并且与【当前接口qps】也对应不起来。如果个别场景确认需要多个接口复用同一个线程池，只能采用每个接口分别计算3值然后相加的方式，从而最大限度地保证安全。 程序运行期间如何修改3值： 比如我们发现活跃线程数远远没有达到核心线程数，就可以适当减小核心线程； 比如我们发现有任务被拒绝，我们就可以适当增加最大线程数或者增加队列长度； 如果我们发现任务等待时间非常长，并且线程活跃数已经到达最大线程数，那就应该考虑是继续增加线程数来提高处理速度，还是要减小队列长度来拒绝部分任务以保护自身服务。 总结🩵 corePoolSize = \\frac{1s需要执行的并发任务数}{1个线程1s可执行任务数} = \\frac{每台机器qps * 当前接口并发任务数}{\\frac{1000ms}{每个任务的平均耗时(ms)}} = \\frac{\\frac{qps}{服务器数量}* 当前接口并发任务数}{\\frac{1000ms}{每个任务的平均耗时(ms)}} maximumPoolSize = 2 * corePoolSize queusSize = maximumPoolSize * \\frac{1000ms * 最多等待的秒数}{每个任务的平均耗时}","tags":["实际开发","Java开发规范","线程池"],"categories":["线程池"]},{"title":"MySQL日期类型浅析","path":"/MySQL/MySQL日期类型浅析/","content":"一、 前言本文主要整理 mysql 提供的五种日期 时间类型 time、date、datetime、timestamp 和 year 的区别，及日期时间类型的精度问题 以下内容基于 MySQL8.0.23 版本进行讲解 二、 mysql 中的日期时间类型mysql 中的日期时间类型有 time、date、datetime、timestamp 和 year 数据类型最小值最大值零值表示date1000-01-019999-12-310000-00-00datetime1000-01-01 00:00:009999-12-31 23:59:590000-00-00 00:00:00[.000000]time-838:59:59838:59:5900:00:00[.000000]timestamp197001010800012038-01-19 03:14:07.999999(准确的来讲应该是 UTC 范围)0000000000000000[000000]year190121550000 每种日期和时间类型都有一个有效范围。如果插入的值超过了这个范围，系统就会报错。对于数据类型TIME、DATETIME和TIMESTAMP，MySQL 5.6.4增加了对小数秒(fsp：[fractional seconds storage])的支持,小数秒可以有6位(微秒)精度 mysql 会对每个数据类型的有效性进行检测，不可以存储不正确、非法的日期，如 date 类型不可以存储 2024-02-31 这个不存在的日期；datetime 中的时分秒不能超过 23:59:59… 等等 2.1、详细解释date: yyyy-mm-dd 格式表示的日期值，date 用于表示年月日，如果实际应用值需要保存年月日就可以使用 date。 MySQL 数据库在存储时会校验 date 类型的正确性。①：必须是 yyyy-MM-dd 格式 ②：必须是合法日期，不能是不存在的日期，如 2024-02-31 datetime: yyyy-mm-dd hh:mm:ss 格式，datetime 用于表示年月日时分秒，是 date 和 time 的组合，并且记录的年份（见上表）比较长久。 time: hh:mm:ss 格式表示的时间值, time 用于表示时分秒 (有负时间表示) timestamp: timestamp 用于表示年月日时分秒，但是记录的年份（见上表）比较短暂最大只能到 2038-01-19 03:14:07.999999(准确的来讲应该是 UTC 范围)。timestamp 和时区相关，更能反映当前时间。当插入日期时，会先转换为本地时区后再存放；当查询日期时，会将日期转换为本地时区后再显示。所以不同时区的人看到的同一时间是不一样的。timestamp 的属性受 Mysql 版本和服务器 SQLMode 的影响较大。如果记录的日期需要让不同时区的人使用，最好使用 timestamp。 year: year 用于表示年份，year 有 2 位（最好使用 4 位）和 4 位格式的年。默认是 4 位。如果实际应用只保存年份，那么用 1 bytes 保存 year 类型完全可以。不但能够节约存储空间，还能提高表的操作效率。 2.2、占用字节在 MySQL 5.6 版本里，对这些类型进行了多项重要的改进：对于数据类型 TIME、DATETIME 和 TIMESTAMP，MySQL 5.6.4 增加了对小数秒 (fsp：[fractional seconds storage]) 的支持。 这些类型现在允许的可选小数部分多达 6 位 (微秒) 精度。MySQL 5.6.5 引入了扩展支持：自动把当前时间戳作为初始值并进行更新。在以前的版本里，这些属性只能用于表里的大部分单个 TIMESTAMP 列。现在，它们可以用于任何 TIMESTAMP 列，并且也可用于 DATETIME 列。MySQL 5.6.6 丢弃了对 YEAR(2) 的支持，取而代之的是允许创建像 YEAR(4) 那样的列。 如果要声明包含小数秒部分的时态类型列，则需要把定义写成 type_name(fsp)，其中，type_name 为 TIME、DATETIME 或 TIMESTAMP，fsp 为小数秒精度。例如，下面的 TIME 列允许 的小数位数分别为 3 位和 6 位： CREATE TABLE `date_test` ( `d3` time(3), `d4` time(6)) ENGINE=InnoDB AUTO_INCREMENT=0 COMMENT=test测试; fsp 值的取值范围必须为 0~6。如果未给定，则默认为 0。更多相关信息在下面会介绍。下图展示的是每一种数据类型的存储空间要求：下图展示的是那些声明中带有小数秒部分的类型所具有的额外存储空间要求： 2.3、time 类型time 类型使用 3 个字节来表示时间。MySQL 中以 HH:MM:SS 的形式显示 Time 类型的值。其中，HH 表示时；MM 表示分，取值范围为 0 ~~ 59；SS 表示秒，取值范围是 0 ~~ 59。Time 类型的范围可以从‘-838：59：59’ ~~ ‘838：59：59’。虽然，小时的范围是 0~~23，但是为了表示某种特殊需要的时间间隔，将 Time 类型的范围扩大了。而且还支持了负值。 Time 类型的字段赋值的表示方法如下：1，‘D HH:MM:SS’格式的字符串表示。其中，D 表示天数，取值范围是 0~~34。保存时，小时的值等于（D*24+HH）。举个例子，输入‘2 11：30：50’，Time 类型会转换为 59：30：50。当然。输入时可以不严格按照这个格式，也可以是‘HH:MM:SS’,‘HH:MM’,‘D HH:MM’,‘D HH’,’SS’等形式。举个例子，输入‘30’，Time 类型会自动转换为 00：00：30。 2，‘HHMMSS’格式的字符串或 HHMMSS 格式的数值表示，例如，输入‘123456’，Time 类型会转换成 12：34：56；输入 123456，Time 类型会转换成 12：34：56。如果输入 0 或者‘0’，那么 TIME 类型会转换为 0000：00：00。 3，使用 current_time 或者 current_time() 或者 now() 输入当前系统时间。 三、datetime 和 timestamp 区别3.1、相同点datetime 和 timestamp 都可以表示 YYYY-MM-DD HH:MM:SS 这种年月日时分秒格式的数据。 并且从 MySQL5.6.4 之后这两者都可以包含秒后的小数部分，精度最高为微妙（6 位）。 这里有一个点需要注意，就是在 MySQL5.6.4 之前，这两个是都表示不了小数的。 3.2、不同点1：存储范围不同 datetime 的存储范围是 1000-01-01 00:00:00.000000 到 9999-12-31 23:59:59.999999，而 timestamp 的范围是 1970-01-01 00:00:01.000000 到 2038-01-19 03:14:07.999999(准确的来讲应该是 UTC 范围) 2：时区相关 datetime 存储与时区无关（准备来说是 datetime 只支持一个时区，就是存储时当前服务器的时区），而 timestamp 存储的是与时区有关。 MySQL 在存储 timestamp 时，会先将时间从当前服务器的时区转换为 UTC（世界协调时）以进行存储，然后查询时从 UTC 转换为当前时区以进行返回。也就是说使用 timestamp 进行存储的时间返回的时候会随着数据库的时区而发生改变。而 datetime 的存储则与时区无关，数据是什么就存储什么，也就返回什么。 timestamp 更适合来记录时间，比如我在东八区时间现在是 2021-06-08 10:23:45， 你在日本（东九区此时时间为 2021-06-08 11:23:45），我和你在聊天，数据库记录了时间，取出来之后，对于我来说时间是 2021-06-08 10:23:45，对于日本的你来说就是 2021-06-08 11:23:45。所以就不用考虑时区的计算了。 3: 存储空间大小 在 5.6.4 之前，datetime 存储占用 8 个字节，而 timestamp 是占用 4 字节；但是在 5.6.4 之后，由于这两个类型允许有小数部分，所以占用的存储空间和以前不同；MySQL 规范规定，datetime 的非小数部分需要 5 个字节，而不是 8 个字节，而 timestamp 的非小数部分是需要 4 个字节，并且这两个部分的小数部分都需要 0 到 3 个字节，具体取决于存储值的小数秒精度。 四、mysql 日期类型中的坑4.1、 time、timestamp、datetime 数据类型四舍五入当前 mysql 版本为 8.0.18 CREATE TABLE `date_test` ( `id` int(20) NOT NULL AUTO_INCREMENT COMMENT 主键id, `d1` date, `d2` datetime, `d3` time, `d4` timestamp, `d5` year, primary key(id)) ENGINE=InnoDB AUTO_INCREMENT=0 COMMENT=test测试; insert into date_test(d2,d4) values(2021-02-23 10:16:55.781,2021-02-23 10:16:55.781); 结论： 当 time、timestamp、datetime 数据类型不指定精度时默认会四舍五入问题解决： 可以设置字段的精度，如 timestamp(3)、timestamp(6) 五、其他疑问问题5.1、date 类型可以存时分秒么？可以存储不存在的日期么？ date 类型只能存储 yyyy-mm-dd 格式的日期，不能存储时分秒字段；date 类型只能存储合法的日期，即必须存在的日期，不能是非法日期，如 2024–02-30 是不存在的，不可以存储 示例： CREATE TABLE `date_test` ( `id` int(20) NOT NULL AUTO_INCREMENT COMMENT 主键id, `d1` date, `d2` datetime, `d3` time, `d4` timestamp, `d5` year, primary key(id)) ENGINE=InnoDB AUTO_INCREMENT=0 COMMENT=test测试;select * from date_test;insert into date_test(d1) values(2024-02-30);","tags":["MySQL"],"categories":["MySQL"]},{"title":"Java时间格式化","path":"/实际开发/Java时间格式化/","content":"1.String.format()format(String format, Object… args) 新字符串使用本地语言环境，制定字符串格式和参数生成格式化的新字符串。 format(Locale locale, String format, Object… args) 使用指定的语言环境，制定字符串格式和参数生成格式化的字符串。 1.1 日期格式化常用的日期转换格式符 转换符说明示例%te一个月中的某一天（1~31）10%td一个月中的第几天（1~31）03%tj一年中的第几天（1~366）020%tb指定语言环境的月份简称Feb（英文）、二月（中文）%tB指定语言环境的月份全称February（英文）、二月（中文）%ta指定语言环境的星期简称Mon（英文）、星期一（中文）%tA指定语言环境的星期全称Monday（星期一）、星期一（中文）%tc包括全部日期和时间信息星期六 六月 10 09:10:20 CST 2023%tY4 位年份2023%ty2 位年份23%tm月份05 实例： public class Demo public static void main(String[] args) Date date = new Date(); String day = String.format(%te, date); System.out.println(今天是2019年8月： + day + 号); String month = String.format(%tb, date); System.out.println(现在是2019年： + month); String xingqi = String.format(%tA, date); System.out.println(今天是： + xingqi); String year = String.format(%tY, date); System.out.println(现在是： + year + 年); 结果： 30号十月今天是：星期三现在是：2024年 1.2 时间格式化常用的时间格式转换符 转换符说明示例%tH2 位数字的 24 时制的小时（00~23）13%tI2 位数字的 12 时制的小时（00~23）01%tM2 位数字的分钟（00~59）05%tS2 位数字的秒数（00~60）12%tL3 位数字的毫秒（000~999）666%tp指定语言环境下的上午或下午标记下午（中文）、pm（英文）%tZ时区缩写形式的字符串CST 实例： public class Demo public static void main(String[] args) Date date = new Date(); String hour = String.format(%tH, date); String minute = String.format(%tM, date); String second = String.format(%tS, date); System.out.println(现在是： + hour + 点 + minute + 分 + second + 秒); System.out.println(##################################); String hour2 = String.format(%tI, date); String pm = String.format(%tp, date); System.out.println(现在是： + pm + hour2 + 点 + minute + 分 + second + 秒); 结果： 现在是：15点59分05秒##################################现在是：下午03点59分05秒 1.3 常见的日期和时间组合的格式转换符说明示例%tF\"年 - 月 - 日\" 格式2021-05-09%tD\"月 / 日 / 年\" 格式05/09/2021%tT\"时：分：秒\" 24 时制08:20:42%tR\"时：分\" 24 时制08:20 public class Demo public static void main(String[] args) Date date = new Date(); String time = String.format(%tc, date); String form = String.format(%tF, date); String form2 = String.format(%tD, date); String form3 = String.format(%tr, date); String form4 = String.format(%tT, date); String form5 = String.format(%tR, date); System.out.println(全部的时间信息是： + time); System.out.println(年-月-日格式： + form); System.out.println(年/月/日格式： + form2); System.out.println(时：分：秒 PM(AM)格式： + form3); System.out.println(时：分：秒格式： + form4); System.out.println(时：分格式： + form5); 结果： 全部的时间信息是：星期三 十月 30 16:00:32 CST 2024年-月-日格式：2024-10-30年/月/日格式：10/30/24时：分：秒 PM(AM)格式：04:00:32 下午时：分：秒格式：16:00:32时：分格式：16:00 2.DateFormat DateFormat 是日期 / 时间格式化的抽象类，它以与语言无关的方式格式化并分析日期或时间。DataFormat 还有一个子类 —— SimpleDateFormat，可用此类格式化日期。其中，最常用的方法是 format() 方法。 实例： public class Demo public static void main(String[] args) SimpleDateFormat sdf1 = new SimpleDateFormat(yy年MM月dd日); SimpleDateFormat sdf2 = new SimpleDateFormat(yyyy年MM月dd日); SimpleDateFormat sdf3 = new SimpleDateFormat(yyyy年MM月dd日 HH时mm分); SimpleDateFormat sdf4 = new SimpleDateFormat(yyyy年MM月dd日 HH时mm分ss秒); SimpleDateFormat sdf5 = new SimpleDateFormat(今年已经过了DDD天，快w个星期，现在是这个月的第W个星期); SimpleDateFormat sdf6 = new SimpleDateFormat(现在是E); String date1 = sdf1.format(new Date()); String date2 = sdf2.format(new Date()); String date3 = sdf3.format(new Date()); String date4 = sdf4.format(new Date()); String date5 = sdf5.format(new Date()); String date6 = sdf6.format(new Date()); System.out.println(date1); System.out.println(date2); System.out.println(date3); System.out.println(date4); System.out.println(date5); System.out.println(date6); 结果： 24年10月30日2024年10月30日2024年10月30日 16时01分2024年10月30日 16时01分35秒今年已经过了304天，快44个星期，现在是这个月的第5个星期现在是星期三 3.SimpleDateFormat 类 在使用 DateFormat 类时，只有固定格式，在 java.text.SimpleDateFormat 包有一个以与语言环境有关的方式来格式化和解析日期的具体类。它允许进行格式化（日期 - 文本）、解析（文本 - 日期）和规范化。 SimpleDateFormat 使得可以选择任何用户定义的日期 - 时间格式的模式。 SimpleDateFormat 类基本构造方法： SimpleDateFormat 类是 DateFormat 类的一个子类。 new SimpleDateFormat(); 默认模式和默认日期格式符号创造。 SimpleDateFormat(String pattern); 使用指定的模式和默认日期格式符号创造。 SimpleDateFormat(String pattern, Locale locale); 使用指定的模式和指定语言环境的默认日期符号构造。 DateFormat 也可以 new SimpleDateFormat 对象。 DateFormat d=new SimpleDateFormat(); 代表日期和时间的模式元素字母： 字母含义示例y年，一个 y 代表一位\"yyy\" 代表 019，\"yyyy\" 代表 2019M月份例如八月，M 代表 8，MM 代表 08w一年中的第几周常用 ww 表示W一个月中的第几周常用 WW 表示d一个月中的第几天常用 dd 表示D一年中的第几天常用 DDD 表示E星期几，用 E 表示星期，根据不同语言环境返回CHINA 表示星期几，US 表示英文缩写a上午或下午am 代表上午，pm 代表下午H一天中的小时数，二十四小时制常用 HH 表示h一天中的小时数，十二小时制常用 hh 表示m分钟数常用 mm 表示s秒数常用 ss 表示S毫秒数常用 SSS 表示 实例： public class FormatDateTime public static void main(String[] args) SimpleDateFormat myFmt=new SimpleDateFormat(yyyy年MM月dd日 HH时mm分ss秒); SimpleDateFormat myFmt1=new SimpleDateFormat(yy/MM/dd HH:mm); SimpleDateFormat myFmt2=new SimpleDateFormat(yyyy-MM-dd HH:mm:ss);//等价于now.toLocaleString() SimpleDateFormat myFmt3=new SimpleDateFormat(yyyy年MM月dd日 HH时mm分ss秒 E ); SimpleDateFormat myFmt4=new SimpleDateFormat( 一年中的第 D 天 一年中第w个星期 一月中第W个星期 在一天中k时 z时区); Date now=new Date(); // 现在的日期 // 按照自定义的时间编排格式 输出时间 System.out.println(myFmt.format(now)); System.out.println(myFmt1.format(now)); System.out.println(myFmt2.format(now)); System.out.println(myFmt3.format(now)); System.out.println(myFmt4.format(now)); System.out.println(now.toGMTString()); System.out.println(now.toLocaleString()); System.out.println(now.toString()); 结果： 2024年10月30日 16时02分43秒24/10/30 16:022024-10-30 16:02:432024年10月30日 16时02分43秒 星期三 一年中的第 304 天 一年中第44个星期 一月中第5个星期 在一天中16时 CST时区30 Oct 2024 08:02:43 GMT2024-10-30 16:02:43Wed Oct 30 16:02:43 CST 2024","tags":["实际开发"],"categories":["实际开发"]},{"title":"时间戳介绍","path":"/常识/时间戳介绍/","content":"时间戳（Timestamp）是表示特定时间点的数值，通常以自 1970 年 1 月 1 日 00:00:00 UTC （世界协调时）以来的秒数或毫秒数来表示。这个时间点被称为 Unix 纪元（Unix epoch）。时间戳广泛用于计算机系统中，用于记录事件发生的精确时间。 时间戳的格式时间戳通常是一个整数或浮点数，例如： Unix 时间戳（秒级）： 1622548800 Unix 时间戳（毫秒级）： 1622548800123 时间戳的用途 记录日志：在日志文件中使用时间戳可以精确记录事件发生的时间，方便日后分析和调试。 数据库记录：在数据库中使用时间戳可以追踪记录的创建和更新时间。 时间计算：通过时间戳可以方便地进行时间差的计算，例如测量程序执行时间、计算事件间隔等。 排序：使用时间戳可以对事件按时间顺序进行排序。 使用时间戳以下是一些常见编程语言中如何使用时间戳的示例： JavaScript // 获取当前时间的时间戳（秒级）const timestampInSeconds = Math.floor(Date.now() / 1000); // 获取当前时间的时间戳（毫秒级）const timestampInMilliseconds = Date.now(); // 将时间戳转换为日期对象const date = new Date(timestampInMilliseconds);console.log(date.toString()); Python import timeimport datetime # 获取当前时间的时间戳（秒级）timestamp_in_seconds = int(time.time()) # 获取当前时间的时间戳（毫秒级）timestamp_in_milliseconds = int(time.time() * 1000) # 将时间戳转换为日期时间对象date = datetime.datetime.fromtimestamp(timestamp_in_seconds)print(date.strftime(%Y-%m-%d %H:%M:%S)) Java import java.time.Instant;import java.time.LocalDateTime;import java.time.ZoneId;import java.time.format.DateTimeFormatter; public class Main public static void main(String[] args) // 获取当前时间的时间戳（秒级） long timestampInSeconds = Instant.now().getEpochSecond(); // 获取当前时间的时间戳（毫秒级） long timestampInMilliseconds = Instant.now().toEpochMilli(); // 将时间戳转换为日期时间对象 LocalDateTime dateTime = LocalDateTime.ofInstant(Instant.ofEpochMilli(timestampInMilliseconds), ZoneId.systemDefault()); DateTimeFormatter formatter = DateTimeFormatter.ofPattern(yyyy-MM-dd HH:mm:ss); System.out.println(dateTime.format(formatter)); 这些例子展示了如何获取当前时间的时间戳，如何将[时间戳转换](https://so.csdn.net/so/search?q=%E6%97%B6%E9%97%B4%E6%88%B3%E8%BD%AC%E6%8D%A2spm=1001.2101.3001.7020)为日期时间对象，并输出为人类可读的格式。时间戳在编程中是非常有用的工具，能够精确记录和处理时间相关的信息。 以下是一些适合中国用户使用的在线时间戳转换工具： 时间戳转换工具地址：[tool.lu/timestamp/] 描述：一个简单易用的时间戳转换工具，支持将时间戳转换为北京时间，以及将北京时间转换为时间戳。 在线工具 - Unix 时间戳转换地址：[bejson.com/convert/unix/] 描述：提供 Unix 时间戳与北京时间的相互转换。 菜鸟工具 - Unix 时间戳地址：[c.runoob.com/front-end/854] 描述：支持 Unix 时间戳与标准时间的转换。 这些工具都提供了简洁易用的界面，可以方便地进行时间戳与北京时间的相互转换。你可以选择一个适合自己的工具进行使用。","categories":["常识"]},{"title":"时间格式化-前后端交互- @JsonFormat 和 @DateTimeFormat","path":"/Spring/时间格式化-前后端交互-JsonFormat-和-DateTimeFormat/","content":"时间格式化 @JsonFormat 和 @DateTimeFormat一、示例代码 先准备一个简单 POJO，拥有 Date 类型的成员变量： @Datapublic class DateEntity private Date date; 再准备一个 Controller，模拟一下前后交互： @RestController@RequestMapping(/date)public class DateController @RequestMapping(/test) public DateEntity getDate(@RequestBody DateEntity dateEntity) System.out.println(入参的date:+dateEntity.getDate()); SimpleDateFormat sdf = new SimpleDateFormat(yyyy-MM-dd HH:mm:ss); String date = sdf.format(dateEntity.getDate()); System.out.println(SimpleDateFormat格式化后的date:+date); DateEntity result = new DateEntity(); result.setDate(new Date()); return result; 创建好 POJO 和 Controller 后，用 Postman 模拟一下请求发送： 结果报错： 2024-10-14 16:00:29.159 WARN 23616 --- [nio-8080-exec-1] .w.s.m.s.DefaultHandlerExceptionResolver : Resolved [org.springframework.http.converter.HttpMessageNotReadableException: JSON parse error: Cannot deserialize value of type `java.util.Date` from String 2025-03-07 23:59:59: not a valid representation (error: Failed to parse Date value 2025-03-07 23:59:59: Cannot parse date 2025-03-07 23:59:59: while it seems to fit format yyyy-MM-ddTHH:mm:ss.SSSX, parsing fails (leniency? null)); nested exception is com.fasterxml.jackson.databind.exc.InvalidFormatException: Cannot deserialize value of type `java.util.Date` from String 2025-03-07 23:59:59: not a valid representation (error: Failed to parse Date value 2025-03-07 23:59:59: Cannot parse date 2025-03-07 23:59:59: while it seems to fit format yyyy-MM-ddTHH:mm:ss.SSSX, parsing fails (leniency? null))EOL at [Source: (org.springframework.util.StreamUtils$NonClosingInputStream); line: 2, column: 12] (through reference chain: com.saikuai.pojo.DateTest[date])] 大概意思就是说 String 类型转换成 Date 类型失败，所以报了 IllegalArgumentException 异常； 二、@JsonFormat 注解 提供者：jackson 作用：可以约束时间的接收格式和响应格式 (接收和响应的都是 JSON 字符串)，将日期类型数据在 JSON 格式和 java.util.Date 对象之间转换。与传输方向没有关系（前端到后端 or 后端到前端都可以使用），注意因为我们是东八区（北京时间），使用时需要加上时区（ timezone = “GMT+8”），不然所得值会比实际时间晚 8 小时； 常用注解属性： 名称作用pattern约定时间格式：pattern=“yyyy-MM-dd HH:mm:ss”timezone指定具体时区： timezone = “GMT+8” or timezone = “Asia/Shanghai” 经过测试使用单独使用 @JsonFormat 注解时需要先通过 @RequestBody 将入参参数映射到实体后，@JsonFormat 注解才能去对时间格式进行约束； POJO 类中也加上了 @JsonFormat 注解： @Datapublic class DateEntity @JsonFormat(pattern = yyyy-MM-dd hh, timezone = GMT+8) private Date date; private String name; private Integer age; 测试： DateTest(date=Fri Mar 07 23:59:59 CST 2025, name=null, age=null)入参的date:Fri Mar 07 23:59:59 CST 2025SimpleDateFormat格式化后的date:2025-03-07 23:59:59 -- responsebody： date: 2024-10-14 16:07:38, name: null, age: null 注意： 对前端入参和 后段出参都做了约束 三、@DateTimeFormat 注解 提供者：Spring 作用：可对 java.util.Date、java.uitl.calendar、java.long.Long 及 Joda 时间类型的属性进行标注，主要处理前端时间类型与后端 pojo 对象中的成员变量进行数据绑定，所约束的时间格式并不会影响后端返回前端的时间类型数据格式； 注意（注意！注意！注意！讲三遍）：前端入参数据的时间格式必须与注解中定义的时间格式相同，不然会报错，如：@DateTimeFormat(pattern = “yyyy-MM-dd HH:mm”) 则入参的格式必须为 “2020-6-4 10:43”； 常用注解属性： 名称作用iso类型为 DateTimeFormat.ISO，常用值：DateTimeFormat.ISO.DATE：格式为 yyyy-MM-ddDateTimeFormat.ISO.DATE_TIME：格式为 yyyy-MM-dd hh:mm:ss.SSSZDateTimeFormat.ISO.TIME：格式为 hh:mm:ss.SSSZDateTimeFormat.ISO.NONE：表示不使用 ISO 格式的时间（默认值）pattern类型为 String，使用自定义时间格式化字符串，如 \"yyyy-MM-dd hh:mm:ss\"style类型为 String，通过样式指定日期时间的格式，由两位字符组成，第一位表示日期的样式，第二位表示时间的格式，以下是几个常用的可选值：S：短日期 / 时间的样式M：中日期 / 时间的样式L：短日期 / 时间的样式F：完整日期 / 时间的样子-：忽略日期或时间的样式默认值 style=“SS” 五、总结 @JsonFormat 和 @DateTimeFormat 区别 @JsonFormat 既可以约束前端传入的时间类型参数格式，也可以约束后端响应前端的时间类型格式； @DateTimeFormat ： 只能约束前端入参时间类型的格式，并不会修改原有的日期对象的格式，如果想要获得期望的日期格式，是需要自己手动转换的； 如果单独使用@DateTimeFormat 时，响应给前端的时间会比实际时间晚 8 个小时（时区原因）。 针对 @DateTimeFormat 做了补充说明，有场景示例比较详情，需要的大佬可以看一下。 传送门：Spring @DateTimeFormat 日期格式化时注解浅析分享","tags":["实际开发","Spring"],"categories":["Spring"]},{"title":"Java开发规范","path":"/Java开发规范/Java开发规范/","content":"1、关于Object 的 equalsObject.equals() 方法是 Java 语言中的一个基础方法，用来比较两个对象是否相等。以下是它的源代码实现： public boolean equals(Object obj) return (this == obj); 我们需要注意的是，Object 的 equals() 方法是根据对象的引用来判断两个对象是否相等的，而不是根据对象的内容。如果我们要比较两个对象的内容是否相等，我们需要重写 equals() 方法，这样就可以根据对象的具体内容来判断它们是否相等了。不再详述。 Java 中的 Objects.equals() 方法实际上是由 Java 中的 Objects 类提供的静态方法，以下是它的源代码： public static boolean equals(Object a, Object b) return (a == b) || (a != null a.equals(b)); 我们需要注意的是，这个方法中使用的是逻辑或运算符 (||)。首先检查第一个条件是否为 true，只有当第一个条件为 false 时才会执行第二个条件。这种方式在 Java 中有时被称为 “short-circuiting”。这里也就是短路或，这种技巧可以用来提高代码的效率，避免执行不必要的操作。 Objects.equals() 方法是 Java 中用于比较两个对象是否相等的一个实用工具方法，它可以处理对象为 null 的情况，避免了因对象为 null 而产生 NullPointerException 的问题。具体来说，如果两个参数都是 null，则返回 true；如果一个参数是 null 而另一个不是 null，则返回 false；否则，调用第一个参数的 equals 方法进行比较。这样就避免了空指针异常了。 总结：Object的equals方法容易抛出空指针异常，应使用常量或确定有值的对象来调用equals。 正： “test”.equals(object) 反： object.equals(“test”) 推荐使用JDK7引入的工具类 java.util.Objects#equals(Object a, Object b) 2、关于基本数据类型与包装数据类型的使用标准 【强制】所有POJO类属性必须使用包装数据类型 【强制】RPC方法的返回值和参数必须使用包装数据类型 【建议】所有局部变量使用基本数据类型 Java基本数据类型 与 包装数据类型详解","tags":["Java开发规范"],"categories":["Java开发规范"]},{"title":"MySQL自增主键与UUID","path":"/MySQL/MySQL自增主键与UUID/","content":"MySQL自增主键自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT 插入新记录的时候可以不指定ID 的值，系统会获取当前的ID 最大值加1 作为下一条记录的ID 值。 也就是说，自增主键的插入数据模式，正好符合 递增插入的场景。每插入一条新记录，都是追加操作，都不涉及挪动其他记录，也不会触发叶子节点的分裂。而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。 除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份成号做主键，还是用自增主键做主键呢？ 由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约20个字节，而如果用整型做主键，则只要4个字节，如果是长整型（bigint）则是8个字节。 显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。 UUID 与 AUTO INCREAMENT对比 UUID做主键 优点 缺点 1 唯一标识，不会考虑重复问题 UUID随机生成，会发生随机IO，影响插入速度，且B+树的页分裂会导致硬盘使用率低 2 可以在应用层生成，提高数据库吞吐率 占用空间较大 3 无需担心业务量泄露的风险 UUID之间比较大小比自增主键ID慢，影响查询速度 注意：InnoDB中主键索引是聚簇索引，如果主键索引是自增ID，只需按照顺序往后排即可，如果是UUID，ID是随机生成的，在数据插入时就造成大量的数据移动，发生随机ID，印象插入速度，造成硬盘的使用率较低 自增主键 优点 缺点 1 字段长度比UUID小 因为是自增，在某些业务场景下，容易被其他人查到业务量 2 数据库自动编号，插入时无需指定ID，按顺序存放，利于检索 发生数据迁移时，或者表合并时麻烦 3 无需担心主键重复问题 在高并发场景下，竞争自增锁会降低数据的吞吐能力","tags":["MySQL"],"categories":["MySQL"]},{"title":"一条SQL的更新语句是如何执行的？","path":"/MySQL/一条SQL的更新语句是如何执行的？/","content":"一条SQL更新语句是如何执行的？相信你还记得，一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后到达存储引擎 那么一条更新语句的执行流程？ create table T(ID int primary key, c int);update T set c = c + 1 where ID = 2;# 从这条更新语句出发讨论 一条SQL更新语句是如何执行的 你可能听DBA同事说，MySQL可以恢复到半个月内任意一秒的状态，惊叹的同时，你是不是心中也会不免好奇，这是怎样做到呢 你执行语句前要先链接数据库，这是连接器的工作。 前面我们说过，在一个表上有更新的时候，跟这张表有关的查询缓存会失效，所以这条语句就会把表T上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。 接下来，分词器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用ID这个索引。然后执行器负责具体执行，找到这一行更新。 与查询流程不一样的是，更新流程还设计两个重要的日志模块，redo log 和 bin log redo log背景：在MySQL里有这样一个问题，如果每一次更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后更新，整个过程IO成本，查找成本都很高。 MySQL设计者 使用WAL技术提升效率， WAL的全称是Write-Ahead-Logging，它的关键点就是先写日志，再写磁盘 具体来说就是，当有一条记录需要更新的时候，InnoDB引擎会先把记录写到redo log里面，并更新内存，这个时候更新就完成了，然后InnoDB引擎会在合适的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做的， InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么这块总共就可以记录4GB的操作，从头开始写到末尾，就又回到开头循环写 write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos和checkpoint之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。 有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 要理解crash-safe这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。 重要的日志模块：binlog前面我们讲过，MySQL整体来看，其实就有两块：一块是Server层，它主要做的是MySQL功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板redo log是InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog（归档日志）。 我想你肯定会问，为什么会有两份日志呢？ 因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。 这两种日志有以下三点不同。 redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。 redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。 redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 有了对这两个日志的概念性理解，我们再来看执行器和InnoDB引擎在执行这个简单的update语句时的内部流程。 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的binlog，并把binlog写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。 这里我给出这个update语句的执行流程图，图中浅色框表示是在InnoDB内部执行的，深色框表示是在执行器中执行的。 两段式提交两阶段提交原理描述： 1、InnoDB redo log 写盘，InnoDB事物进入prepare状态。 2、如果prepare成功，binlog 写盘，那么再继续将事物日志持久化到binlog，如果持久化成功，那么InnoDB事物则会进入commit状态（在redo log里面写一个commit记录）备注： 每个事物binlog 的末尾，会记录一个XID event， 标志着事物是否是否提交成功，也就是说，回滚过程中，binlog最后一个XID event之后的内容都应该会被清除 为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。 由于redo log 和 bin log 是独立的逻辑，如果不用两阶段提交，要么就是先写完redo log 再写 bin log 或者采用反过来的顺讯。我们看看这两种方式有什么问题。 仍然用前面的update语句来做例子。假设当前ID = 2的行，字段c的值是0，再假设执行update语句的过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢？ 1、先写redo log 后写 binlog。 假设在redo log写完，binlog 还没有写完的时候，MySQL进程异常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行c的值是1. 但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份日志的时候，存起来的binlog里面就没有这条语句。然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个binlog丢失，这个临时哭就会少了这一次更新，恢复出来的这一行c的值就是0，与原库不同 2、先写binlog 后写redo log。 如果在binlog写完之后crash ，由于 redo log 还没写，崩溃恢复后这个食物无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日志。所以，之后用binlog来恢复的时候就多了一个事物出来，恢复出来的这一行c的值就是1，与原库不同。 —- 如果不使用“两阶段提交” ，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致 简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。","tags":["MySQL"],"categories":["MySQL"]},{"title":"MySQL使用B+树不使用跳表?(Reids中Zset使用跳表不使用B+树)","path":"/MySQL/MySQL使用B-树不使用跳表-Reids中Zset使用跳表不使用B-树/","content":"在我们的印象中，MySQL 数据表里无非就是存储一行行的数据。跟个 [直接遍历这一行行数据，性能就是 O(n)，比较慢。为了加速查询，使用了 B + 树来做索引，将查询性能优化到了 O(lg(n))。 但问题就来了，查询数据性能在 lg(n) 级别的数据结构有很多，比如redis的zset里用到的跳表，也是lg(n)，并且实现还贼简单。 那为什么mysql的索引，不使用跳表呢？ 一、B+ 树 B + 树的结构。 B + 树查询过程 如上图，一般 B + 树是由多个页组成的多层级结构，每个页 16Kb，对于主键索引来说，最末级的叶子结点放行数据，非叶子结点放的则是索引信息（主键 id 和页号），用于加速查询。 比方说我们想要查找行数据 5。会先从顶层页的 record 们入手。record 里包含了主键 id 和页号（页地址）。关注黄色的箭头，向左最小 id 是 1，向右最小 id 是 7。那 id=5 的数据如果存在，那必定在左边箭头。于是顺着的 record 的页地址就到了 6 号数据页里，再判断 id=54，所以肯定在右边的数据页里，于是加载 105 号数据页。 在 105 号数据页里，虽然有多行数据，但也不是挨个遍历的，数据页内还有个页目录的信息，它可以通过二分查找的方式加速查询行数据，于是找到 id=5 的数据行，完成查询。 从上面可以看出，B + 树利用了空间换时间的方式（构造了一批非叶子结点用于存放索引信息），将查询时间复杂度从 O(n) 优化为 O(lg(n))。 二、跳表（具体查询方式见 Redis—跳表原理） 看完 B + 树，我们再来看下跳表是怎么来的。 同样的，还是为了存储一行行的数据。 我们可以将它们用链表串起来。 单链表 想要查询链表中的其中一个结点，时间复杂度是 O(n)，这谁顶得住，于是将部分链表结点提出来，再构建出一个新的链表。 两层跳表 这样当我想要查询一个数据的时候，我先查上层的链表，就很容易知道数据落在哪个范围，然后跳到下一个层级里进行查询。这样就把搜索范围一下子缩小了一大半。 比如查询 id=10 的数据，我们先在上层遍历，依次判断 1,6,12，很快就可以判断出 10 在 6 到 12 之间，然后往下一跳，就可以在遍历 6,7,8,9,10 之后，确定 id=10 的位置。直接将查询范围从原来的 1 到 10，变成现在的 1,6,7,8,9,10，算是砍半了。 两层跳表查找 id 为 10 的数据 既然两层链表就直接将查询范围砍半了，那我多加几层，岂不妙哉？ 于是跳表就这样变成了多层。 三层跳表 如果还是查询 id=10 的数据，就只需要查询 1,6,9,10 就能找到，比两层的时候更快一些。 三层跳表查询 id 为 10 的数据 三、插入对比可以看出，跳表也是通过牺牲空间换取时间的方式提升查询性能。时间复杂度都是 lg(n)。 从上面可以看到，B + 树和跳表的最下面一层，都包含了所有的数据，且都是顺序的，适合用于范围查询。往上的层级都是构建出来用于提升搜索性能的。这两者实在是太像了。但他们两者在新增和删除数据时，还是有些区别的。下面我们以新增数据为例聊一下。 B + 树本质上是一种多叉平衡二叉树。 关键在于 “ 平衡 “ 这两个字，对于多叉树结构来说，它的含义是子树们的高度层级尽量一致（一般最多差一个层级），这样在搜索的时候，不管是到哪个子树分支，搜索次数都差不了太多。 当数据库表不断插入新的数据时，为了维持 B + 树的平衡，B + 树会不断分裂调整数据页。 我们知道 B + 树分为叶子结点和非叶子结点。 当插入一条数据时，叶子结点和它上层的索引结点（非叶子结点）最大容量都是 16k，它们都有可能会满。 为了简化问题，我们假设一个数据页只能放三条行数据或索引。 加入一条数据，根据数据页会不会满，分为三种情况。 叶子结点和索引结点都没满。这种情况最简单，直接插入到叶子结点中就好了。 叶子和非叶子都未满 叶子结点满了，但索引结点没满。此时需要拆分叶子结点，同时索引结点要增加新的索引信息。 叶子满了但非叶子未满. drawio 叶子结点满了，且索引结点也满了。叶子和索引结点都要拆分，同时往上还要再加一层索引。 叶子和非叶子都满了 从上面可以看到，只有在叶子和索引结点都满了的情况下，B + 树才会考虑加入一层新的结点。 而从之前的文章知道，要把三层 B + 树塞满，那大概需要 2kw 左右的数据。 跳表同样也是很多层，新增一个数据时，最底层的链表需要插入数据。 此时，是否需要在上面的几层中加入数据做索引呢？ 这个就纯靠随机函数了。 理论上为了达到二分的效果，每一层的结点数需要是下一层结点数的二分之一。 也就是说现在有一个新的数据插入了，它有 50% 的概率需要在第二层加入索引，有 25% 的概率需要在第三层加个索引，以此类推，直到最顶层。 举个例子，如果跳表中插入数据 id=6，且随机函数返回第三层（有 25% 的概率），那就需要在跳表的最底层到第三层都插入数据。 跳表插入数据 如果这个随机函数设计成上面这样，当数据量样本足够大的时候，数据的分布就符合我们理想中的 “二分”。 跟上面 B + 树不一样，跳表是否新增层数，纯粹靠随机函数，根本不关心前后上下结点。 好了，基础科普也结束了，我们可以进入正题了。 四、总结（重点！！！）B + 树是多叉树结构，每个结点都是一个 16k 的数据页，能存放较多索引信息，所以扇出很高。三层左右就可以存储 2kw 左右的数据（知道结论就行，想知道原因可以看之前的文章）。也就是说查询一次数据，如果这些数据页都在磁盘里，那么最多需要查询三次磁盘 IO。 跳表是链表结构，一条数据一个结点，如果最底层要存放 2kw 数据，且每次查询都要能达到二分查找的效果，2kw 大概在 2 的 24 次方左右，所以，跳表大概高度在 24 层左右。最坏情况下，这 24 层数据会分散在不同的数据页里，也即是查一次数据会经历 24 次磁盘 IO。 因此存放同样量级的数据，B + 树的高度比跳表的要少，如果放在 mysql 数据库上来说，就是磁盘 IO 次数更少，因此 B + 树查询更快。 而针对写操作，B + 树需要拆分合并索引数据页，跳表则独立插入，并根据随机函数确定层数，没有旋转和维持平衡的开销，因此跳表的写入性能会比 B + 树要好。 其实，mysql 的存储引擎是可以换的，以前是 myisam，后来才有的 innodb，它们底层索引用的都是 B + 树。也就是说，你完全可以造一个索引为跳表的存储引擎装到 mysql 里。事实上，facebook 造了个 rocksDB 的存储引擎，里面就用了跳表。直接说结论，它的写入性能确实是比 innodb 要好，但读性能确实比 innodb 要差不少。 五、为什么Redis 里面用跳表 而 MySQL里面用B+树？（关键）redis 支持多种数据结构，里面有个有序集合，也叫 ZSET。内部实现就是跳表。那为什么要用跳表而不用 B + 树等结构呢？ 大家知道，redis 是纯纯的内存数据库。 进行读写数据都是操作内存，跟磁盘没啥关系，因此也不存在磁盘 IO 了，所以层高就不再是跳表的劣势了。 并且前面也提到 B + 树是有一系列合并拆分操作的，换成红黑树或者其他 AVL 树的话也是各种旋转，目的也是为了保持树的平衡。 而跳表插入数据时，只需要随机一下，就知道自己要不要往上加索引，根本不用考虑前后结点的感受，也就少了旋转平衡的开销。 因此，redis 选了跳表，而不是 B + 树。 B + 树是多叉平衡搜索树，扇出高，只需要 3 层左右就能存放 2kw 左右的数据，同样情况下跳表则需要 24 层左右，假设层高对应磁盘 IO，那么 B + 树的读性能会比跳表要好，因此 mysql 选了 B + 树做索引。 redis 的读写全在内存里进行操作，不涉及磁盘 IO，同时跳表实现简单，相比 B + 树、AVL 树、少了旋转树结构的开销，因此 redis 使用跳表来实现 ZSET，而不是树结构。 存储引擎 RocksDB 内部使用了跳表，对比使用 B + 树的 innodb，虽然写性能更好，但读性能属实差了些。在读多写少的场景下，B + 树依旧 YYDS。","tags":["MySQL"],"categories":["MySQL"]},{"title":"Java日志框架--转载","path":"/日志框架/Java日志框架/","content":"随着互联网和大数据的迅猛发展，分布式日志系统和日志分析系统已广泛应用，几乎所有应用程序都使用各种日志框架记录程序运行信息。因此，作为工程师，了解主流的日志记录框架非常重要。虽然应用程序的运行结果不受日志的有无影响，但没有日志的应用程序是不完整的，甚至可以说是有缺陷的。优秀的日志系统可以记录操作轨迹、监控系统运行状态和解决系统故障。 目前常用的日志框架有 Log4j，Log4j 2，Commons Logging，Slf4j，Logback，JUL。这些日志框架可以分为两种类型：门面日志和日志系统。 日志门面日志门面（Logging Facade 是一种设计模式，用于在应用程序中实现日志记录的抽象层。它提供了一组统一的接口和方法，即相应的 API，而不提供具体的接口实现。日志门面在使用时，可以动态或者静态地指定具体的日志框架实现，解除了接口和实现的耦合，使用户可以灵活地选择日志的具体实现框架。 日志系统日志系统（Logging System 是指用于记录和管理应用程序运行时产生的日志信息的软件工具或框架。与日志门面相对，它提供了具体的日志接口实现，应用程序通过它执行日志打印的功能，如日志级别管理、日志格式化、日志输出目标设置等。常见的日志系统包括 Log4j、Logback、Java Util Logging 等。 通过使用日志门面，我们可以在应用程序中使用统一的 API 进行日志记录，而具体的日志实现可以根据需要选择和配置。这样，我们可以根据项目需求和团队喜好来灵活选择、切换和配置日志系统，而不会对应用程序代码造成太大影响。 避免环形依赖Slf4j 的作者 Ceki Gülcü 当年因为觉得 Commons-Logging 的 API 设计的不好，性能也不够高，因而设计了 Slf4j。而他为了 Slf4j 能够兼容各种类型的日志系统实现，还设计了相当多的 adapter 和 bridge 来连接，如下图所示： 鉴于此，在引入日志框架依赖的时候要尽力避免，比如以下组合就不能同时出现： •jcl-over-slf4j 和 slf4j-jcl •log4j-over-slf4j 和 slf4j-log4j12 •jul-to-slf4j 和 slf4j-jdk14 日志框架的使用选择常用的组合使用方式是 Slf4j Logback 组合使用，Commons Logging Log4j 组合使用。 推荐： Slf4j Logback 原因： Slf4j 实现机制决定 Slf4j 限制较少，使用范围更广。相较于 Commons-Logging，Slf4j 在编译期间便静态绑定本地的 Log 库，其通用性要好得多； Logback 拥有更好的性能。Logback 声称：某些关键操作，比如判定是否记录一条日志语句的操作，其性能得到了显著的提高，这个操作在 Logback 中只需 3 ns，而在 Log4j 则需要 30 ns； Slf4j 支持参数化，使用占位符号，代码更为简洁，如下例子： // 在使用 Commons-Logging 时，通常的做法是 if(log.isDebugEnabled()) log.debug(User name： + user.getName() + buy goods id ： + good.getId()); // 在 Slf4j 阵营，你只需这么做： log.debug(User name： ,buy goods id ：, user.getName(),good.getId()); Logback 的所有文档是免费提供的，Log4j 只提供部分免费文档而需要用户去购买付费文档； MDC (Mapped Diagnostic Contexts) 用 Filter，将当前用户名等业务信息放入 MDC 中，在日志 format 定义中即可使用该变量。具体而言，在诊断问题时，通常需要打出日志。如果使用 Log4j，则只能降低日志级别，但是这样会打出大量的日志，影响应用性能；如果使用 Logback，保持原定日志级别而过滤某种特殊情况，如 Alice 这个用户登录，日志将打在 DEBUG 级别而其它用户可以继续打在 WARN 级别。实现这个功能只需加 4 行 XML 配置； 自动压缩日志。RollingFileAppender 在产生新文件的时候，会自动压缩已经打出来的日志文件。压缩过程是异步的，因此在压缩过程中应用几乎不会受影响。 maven 依赖pom.xml !--日志框架接口--dependency groupIdorg.slf4j/groupId artifactIdslf4j-api/artifactId/dependency!--日志框架接口实现--dependency groupIdch.qos.logback/groupId artifactIdlogback-classic/artifactId/dependency!--日志框架核心组件--dependency groupIdch.qos.logback/groupId artifactIdlogback-core/artifactId/dependency!--自动化注解工具--dependency groupIdorg.projectlombok/groupId artifactIdlombok/artifactId version1.18.16/version/dependency 配置文件logback.xml ?xml version=1.0 encoding=UTF-8?configuration !--默认日志配置-- include resource=org/springframework/boot/logging/logback/defaults.xml/ !-- 控制台日志 -- appender name=CONSOLE class=ch.qos.logback.core.ConsoleAppender encoder charset=UTF-8 pattern$CONSOLE_LOG_PATTERN/pattern /encoder /appender !-- Info日志 -- appender name=FILE-INFO class=ch.qos.logback.core.rolling.RollingFileAppender file$LOG_PATH/$LOG_FILE-info.log/file appendtrue/append filter class=ch.qos.logback.classic.filter.LevelFilter levelINFO/level onMatchACCEPT/onMatch onMismatchNEUTRAL/onMismatch /filter rollingPolicy class=ch.qos.logback.core.rolling.TimeBasedRollingPolicy fileNamePattern$LOG_PATH/$LOG_FILE-info-%dyyyy-MM-dd.%i.log/fileNamePattern !-- 日志文件的路径和名称 -- timeBasedFileNamingAndTriggeringPolicy class=ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP maxFileSize200MB/maxFileSize !-- 单个日志文件的最大大小 -- /timeBasedFileNamingAndTriggeringPolicy maxHistory15/maxHistory !-- 保留的历史日志文件数量 -- totalSizeCap2GB/totalSizeCap !-- 所有日志文件的总大小上限 -- cleanHistoryOnStarttrue/cleanHistoryOnStart !-- 在启动时清除历史日志文件 -- /rollingPolicy encoder charset=UTF-8 pattern$FILE_LOG_PATTERN/pattern /encoder /appender !-- Warn日志 -- appender name=FILE-WARN class=ch.qos.logback.core.rolling.RollingFileAppender file$LOG_PATH/$LOG_FILE-warn.log/file appendtrue/append filter class=ch.qos.logback.classic.filter.LevelFilter levelWARN/level onMatchACCEPT/onMatch onMismatchDENY/onMismatch /filter rollingPolicy class=ch.qos.logback.core.rolling.TimeBasedRollingPolicy fileNamePattern$LOG_PATH/$LOG_FILE-warn-%dyyyy-MM-dd.%i.log/fileNamePattern !-- 日志文件的路径和名称 -- timeBasedFileNamingAndTriggeringPolicy class=ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP maxFileSize200MB/maxFileSize !-- 单个日志文件的最大大小 -- /timeBasedFileNamingAndTriggeringPolicy maxHistory15/maxHistory !-- 保留的历史日志文件数量 -- totalSizeCap2GB/totalSizeCap !-- 所有日志文件的总大小上限 -- cleanHistoryOnStarttrue/cleanHistoryOnStart !-- 在启动时清除历史日志文件 -- /rollingPolicy encoder charset=UTF-8 pattern$FILE_LOG_PATTERN/pattern /encoder /appender !-- Error日志 -- appender name=FILE-ERROR class=ch.qos.logback.core.rolling.RollingFileAppender file$LOG_PATH/$LOG_FILE-error.log/file appendtrue/append filter class=ch.qos.logback.classic.filter.LevelFilter levelERROR/level onMatchACCEPT/onMatch onMismatchDENY/onMismatch /filter rollingPolicy class=ch.qos.logback.core.rolling.TimeBasedRollingPolicy fileNamePattern$LOG_PATH/$LOG_FILE-error-%dyyyy-MM-dd.%i.log/fileNamePattern timeBasedFileNamingAndTriggeringPolicy class=ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP maxFileSize200MB/maxFileSize /timeBasedFileNamingAndTriggeringPolicy maxHistory15/maxHistory totalSizeCap2GB/totalSizeCap cleanHistoryOnStarttrue/cleanHistoryOnStart /rollingPolicy encoder charset=UTF-8 pattern$FILE_LOG_PATTERN/pattern /encoder /appender !-- 异步输出 -- appender name=info-asyn class=ch.qos.logback.classic.AsyncAppender appender-ref ref=FILE-INFO/ queueSize512/queueSize !-- 异步队列的大小 -- /appender appender name=warn-asyn class=ch.qos.logback.classic.AsyncAppender appender-ref ref=FILE-WARN/ queueSize512/queueSize !-- 异步队列的大小 -- /appender appender name=error-asyn class=ch.qos.logback.classic.AsyncAppender appender-ref ref=FILE-ERROR/ queueSize512/queueSize /appender !-- 应用日志 -- logger name=com.improve.fuqige.bronze additivity=false appender-ref ref=CONSOLE/ appender-ref ref=FILE-INFO/ appender-ref ref=FILE-WARN/ appender-ref ref=FILE-ERROR/ /logger !-- 总日志出口 -- root level=$logging.level.root appender-ref ref=CONSOLE/ appender-ref ref=info-asyn/ appender-ref ref=warn-asyn/ appender-ref ref=error-asyn/ /root/configuration applicantion.properties logging.file=fuqige-bronzelogging.path=XXXXXX/Logs/XXXXXXlogging.level.root=infologging.level.com.improve.fuqige.bronze=infologging.pattern.console=%cyan(%dyyyy-MM-dd HH:mm:ss.SSS) %yellow([%thread]) %highlight(%-5level) %boldGreen(%logger80[LineNumber:%L]): %highlight(%msg%n)logging.pattern.file=%dyyyy-MM-dd HH:mm:ss.SSS [%XrequestId] %-5level --- [%thread] %logger80[LineNumber:%L]: %msg%n 测试用例@Slf4j@RestController@RequestMapping(/test)public class TestController @GetMapping(/hello) public String hello() log.info(进来了!); log.warn(进来了!); log.error(进来了!); return hello, world! requestId= + MDC.get(requestId); 参考资料 Java 日志框架： zhuanlan.zhihu.com/p/365154773 SLF4J 框架常见的用法和最佳实践： juejin.cn/post/721556… 作者：京东零售 张洪 来源：京东云开发者社区 转载请注明来源","tags":["日志框架"],"categories":["日志框架"]},{"title":"KafKa基础原理","path":"/KafKa/KafKa基础原理/","content":"一、概念理解Kafka 是最初由 Linkedin 公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于 zookeeper 协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于 hadoop 的批处理系统、低延迟的实时系统、storm/Spark 流式处理引擎，web/nginx 日志、访问日志，消息服务等等，用 scala 语言编写，Linkedin 于 2010 年贡献给了 Apache 基金会并成为顶级开源 项目。 （1）产生背景当今社会各种应用系统诸如商业、社交、搜索、浏览等像信息工厂一样不断的生产出各种信息，在大数据时代，我们面临如下几个挑战： 如何收集这些巨大的信息 如何分析它 如何及时做到如上两点 以上几个挑战形成了一个业务需求模型，即生产者生产（produce）各种信息，消费者消费（consume）（处理分析）这些信息，而在生产者与消费者之间，需要一个沟通两者的桥梁 - 消息系统。从一个微观层面来说，这种需求也可理解为不同的系统之间如何传递消息。 Kafka 诞生 Kafka 由 linked-in 开源 kafka - 即是解决上述这类问题的一个框架，它实现了生产者和消费者之间的无缝连接。 kafka - 高产出的分布式消息系统 (A high-throughput distributed messaging system) （2）Kafka 的特性 特性 分布式 高性能 持久性和扩展行 多分区 高吞吐量 数据可持久化 多副本 低延迟 容错性 多订阅者 高并发 支持水平在线扩展 基于ZooKeeper调度 时间复杂度O（1） 消息自动平衡 高吞吐量、低延迟：kafka 每秒可以处理几十万条消息，它的延迟最低只有几毫秒 可扩展性：kafka 集群支持热扩展 持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失 容错性：允许集群中节点失败（若副本数量为 n, 则允许 n-1 个节点失败） 高并发：支持数千个客户端同时读写 （3）Kafka 场景应用 日志收集：一个公司可以用 Kafka 可以收集各种服务的 log，通过 kafka 以统一接口服务的方式开放给各种 consumer，例如 hadoop、Hbase、Solr 等。 消息系统：解耦和生产者和消费者、缓存消息等。 用户活动跟踪：Kafka 经常被用来记录 web 用户或者 app 用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到 kafka 的 topic 中，然后订阅者通过订阅这些 topic 来做实时的监控分析，或者装载到 hadoop、数据仓库中做离线分析和挖掘。 运营指标：Kafka 也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。 流式处理：比如 spark streaming 和 storm 事件源 （4）Kafka 一些重要设计思想 Consumergroup：KafKa是按消费组来消费消息的，一个消费组下面的所有机器可以组成一个Consumer Group，每条消息只能被该Consumer Group一个Consumer消费，不容的Consumer Group可以消费同一条消息 消息状态：在 Kafka 中，消息的状态被保存在 consumer 中，broker 不会关心哪个消息被消费了被谁消费了，只记录一个 offset 值（指向 partition 中下一个要被消费的消息位置），这就意味着如果 consumer 处理不好的话，broker 上的一个消息可能会被消费多次。 消息持久化：Kafka 中会把消息持久化到本地文件系统中，并且保持极高的效率。 消息有效期：Kafka 会长久保留其中的消息，以便 consumer 可以多次消费，当然其中很多细节是可配置的。 批量发送：Kafka 支持以消息集合为单位进行批量发送，以提高 push 效率。 push-and-pull :Kafka 中的 Producer 和 consumer 采用的是 push-and-pull 模式，即 Producer 只管向 broker push 消息，consumer 只管从 broker pull 消息，两者对消息的生产和消费是异步的。 Kafka 集群中 broker 之间的关系：不是主从关系，各个 broker 在集群中地位一样，我们可以随意的增加或删除任何一个 broker 节点。 负载均衡方面： Kafka 提供了一个 metadata API 来管理 broker 之间的负载（对 Kafka0.8.x 而言，对于 0.7.x 主要靠 zookeeper 来实现负载均衡）。 同步异步：Producer 采用异步 push 方式，极大提高 Kafka 系统的吞吐率（可以通过参数控制是采用同步还是异步方式）。 分区机制 partition：Kafka 的 broker 端支持消息分区，Producer 可以决定把消息发到哪个分区，在一个分区中消息的顺序就是 Producer 发送消息的顺序，一个主题中可以有多个分区，具体分区的数量是可配置的。分区的意义很重大，后面的内容会逐渐体现。 离线数据装载：Kafka 由于对可拓展的数据持久化的支持，它也非常适合向 Hadoop 或者数据仓库中进行数据装载。 插件支持：现在不少活跃的社区已经开发出不少插件来拓展 Kafka 的功能，如用来配合 Storm、Hadoop、flume 相关的插件。 二、消息队列通信的模式（1）点对点模式 如上图所示，点对点模式通常是基于拉取或者轮询的消息传送模型，这个模型的特点是发送到队列的消息被一个且只有一个消费者进行处理。生产者将消息放入消息队列后，由消费者主动的去拉取消息进行消费。点对点模型的的优点是消费者拉取消息的频率可以由自己控制。但是消息队列是否有消息需要消费，在消费者端无法感知，所以在消费者端需要额外的线程去监控。 （2）发布订阅模式 如上图所示，发布订阅模式是一个基于消息送的消息传送模型，改模型可以有多种不同的订阅者。生产者将消息放入消息队列后，队列会将消息推送给订阅过该类消息的消费者（类似微信公众号）。由于是消费者被动接收推送，所以无需感知消息队列是否有待消费的消息！但是 consumer1、consumer2、consumer3 由于机器性能不一样，所以处理消息的能力也会不一样，但消息队列却无法感知消费者消费的速度！所以推送的速度成了发布订阅模模式的一个问题！假设三个消费者处理速度分别是 8M/s、5M/s、2M/s，如果队列推送的速度为 5M/s，则 consumer3 无法承受！如果队列推送的速度为 2M/s，则 consumer1、consumer2 会出现资源的极大浪费！ 三、Kafka 的架构原理（1）基础架构与名词解释 在这里插入图片描述 Producer：Producer 即生产者，消息的产生者，是消息的入口。 Broker： 一个Kafka集群中的一台服务器就是一个Broker，Broker可以水平无限扩展，同一个Topic中的消息可以分布在多个Broker中。 Topic：消息的主题，可以理解为消息的分类，kafka 的数据就保存在 topic。在每个 broker 上都可以创建多个 topic。 Partition：Topic 的分区，每个 topic 可以有多个分区，分区的作用是做负载，提高 kafka 的吞吐量。同一个 topic 在不同的分区的数据是不重复的，partition 的表现形式就是一个一个的文件夹！ Replication: 每一个分区都有多个副本，副本的作用是做备胎。当主分区（Leader）故障的时候会选择一个备胎（Follower）上位，成为 Leader。在 kafka 中默认副本的最大数量是 10 个，且副本的数量不能大于 Broker 的数量，follower 和 leader 绝对是在不同的机器，同一机器对同一个分区也只可能存放一个副本（包括自己）。 Message：每一条发送的消息主体。 Consumer：消费者，即消息的消费方，是消息的出口。 Consumer Group：我们可以将多个消费组组成一个消费者组，在 kafka 的设计中同一个分区的数据只能被消费者组中的某一个消费者消费。同一个消费者组的消费者可以消费同一个 topic 的不同分区的数据，这也是为了提高 kafka 的吞吐量！ Zookeeper：kafka 集群依赖 zookeeper 来保存集群的的元信息，来保证系统的可用性。 （2）工作流程分析（1）发送数据我们看上面的架构图中，producer 就是生产者，是数据的入口。注意看图中的红色箭头，Producer 在写入数据的时候永远的找 leader，不会直接将数据写入 follower！那 leader 怎么找呢？写入的流程又是什么样的呢？我们看下图： 发送的流程就在图中已经说明了，就不单独在文字列出来了！需要注意的一点是，消息写入 leader 后，follower 是主动的去 leader 进行同步的！producer 采用 push 模式将数据发布到 broker，每条消息追加到分区中，顺序写入磁盘，所以保证同一分区内的数据是有序的！写入示意图如下： 在这里插入图片描述上面说到数据会写入到不同的分区，那 kafka 为什么要做分区呢？相信大家应该也能猜到，分区的主要目的是： 方便扩展：因为一个 topic 可以有多个 partition，所以我们可以通过扩展机器去轻松的应对日益增长的数据量。 提高并发：以 partition 为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率。 熟悉负载均衡的朋友应该知道，当我们向某个服务器发送请求的时候，服务端可能会对请求做一个负载，将流量分发到不同的服务器，那在 kafka 中，如果某个 topic 有多个 partition，producer 又怎么知道该将数据发往哪个 partition 呢？ kafka 中有几个原则： partition 在写入的时候可以指定需要写入的 partition，如果有指定，则写入对应的 partition。 如果没有指定 partition，但是设置了数据的 key，则会根据 key 的值 hash 出一个 partition。 如果既没指定 partition，又没有设置 key，则会轮询选出一个 partition。 保证消息不丢失是一个消息队列中间件的基本保证，那 producer 在向 kafka 写入消息的时候，怎么保证消息不丢失呢？其实上面的写入流程图中有描述出来，那就是通过 ACK 应答机制！在生产者向队列写入数据的时候可以设置参数来确定是否确认 kafka 接收到数据，这个参数可设置的值为 0、1、all。 0 代表 producer 往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。 1 代表 producer 往集群发送数据只要 leader 应答就可以发送下一条，只确保 leader 发送成功。 all 代表 producer 往集群发送数据需要所有的 follower 都完成从 leader 的同步才会发送下一条，确保 leader 发送成功和所有的副本都完成备份。安全性最高，但是效率最低。 最后要注意的是，如果往不存在的 topic 写数据，能不能写入成功呢？kafka 会自动创建 topic，分区和副本的数量根据默认配置都是 1。 （2）保存数据Producer 将数据写入 kafka 后，集群就需要对数据进行保存了！kafka 将数据保存在磁盘，可能在我们的一般的认知里，写入磁盘是比较耗时的操作，不适合这种高并发的组件。Kafka 初始会单独开辟一块磁盘空间，顺序写入数据（效率比随机写入高）。 （1）Partition 结构前面说过了每个 topic 都可以分为一个或多个 partition，如果你觉得 topic 比较抽象，那 partition 就是比较具体的东西了！Partition 在服务器上的表现形式就是一个一个的文件夹，每个 partition 的文件夹下面会有多组 segment 文件，每组 segment 文件又包含. index 文件、.log 文件、.timeindex 文件（早期版本中没有）三个文件， log 文件就实际是存储 message 的地方，而 index 和 timeindex 文件为索引文件，用于检索消息。 如上图，这个 partition 有三组 segment 文件，每个 log 文件的大小是一样的，但是存储的 message 数量是不一定相等的（每条的 message 大小不一致）。文件的命名是以该 segment 最小 offset 来命名的，如 000.index 存储 offset 为 0~368795 的消息，kafka 就是利用分段 + 索引的方式来解决查找效率的问题。 （2）Message 结构上面说到 log 文件就实际是存储 message 的地方，我们在 producer 往 kafka 写入的也是一条一条的 message，那存储在 log 中的 message 是什么样子的呢？消息主要包含消息体、消息大小、offset、压缩类型…… 等等！我们重点需要知道的是下面三个： offset：offset 是一个占 8byte 的有序 id 号，它可以唯一确定每条消息在 parition 内的位置！ 消息大小：消息大小占用 4byte，用于描述消息的大小。 消息体：消息体存放的是实际的消息数据（被压缩过），占用的空间根据具体的消息而不一样。 （3）存储策略无论消息是否被消费，kafka 都会保存所有的消息。那对于旧数据有什么删除策略呢？ 基于时间，默认配置是 168 小时（7 天）。 基于大小，默认配置是 1073741824。 需要注意的是，kafka 读取特定消息的时间复杂度是 O(1)，所以这里删除过期的文件并不会提高 kafka 的性能！ （3）消费数据消息存储在 log 文件后，消费者就可以进行消费了。在讲消息队列通信的两种模式的时候讲到过点对点模式和发布订阅模式。Kafka 采用的是发布订阅模式，消费者主动的去 kafka 集群拉取消息，与 producer 相同的是，消费者在拉取消息的时候也是找 leader 去拉取。 图示是消费者组内的消费者小于 partition 数量的情况，所以会出现某个消费者消费多个 partition 数据的情况，消费的速度也就不及只处理一个 partition 的消费者的处理速度！如果是消费者组的消费者多于 partition 的数量，那会不会出现多个消费者消费同一个 partition 的数据呢？上面已经提到过不会出现这种情况！多出来的消费者不消费任何 partition 的数据。所以在实际的应用中， 建议消费者组的 consumer 的数量与 partition 的数量一致！ 在保存数据的小节里面，我们聊到了 partition 划分为多组 segment，每个 segment 又包含. log、.index、.timeindex 文件，存放的每条 message 包含 offset、消息大小、消息体…… 我们多次提到 segment 和 offset，查找消息的时候是怎么利用 segment+offset 配合查找的呢？假如现在需要查找一个 offset 为 368801 的 message 是什么样的过程呢？我们先看看下面的图： 先找到 offset 的 368801message 所在的 segment 文件（利用二分法查找），这里找到的就是在第二个 segment 文件。 打开找到的 segment 中的. index 文件（也就是 368796.index 文件，该文件起始偏移量为 368796+1，我们要查找的 offset 为 368801 的 message 在该 index 内的偏移量为 368796+5=368801，所以这里要查找的相对 offset 为 5）。由于该文件采用的是稀疏索引的方式存储着相对 offset 及对应 message 物理偏移量的关系，所以直接找相对 offset 为 5 的索引找不到，这里同样利用二分法查找相对 offset 小于或者等于指定的相对 offset 的索引条目中最大的那个相对 offset，所以找到的是相对 offset 为 4 的这个索引。 根据找到的相对 offset 为 4 的索引确定 message 存储的物理偏移位置为 256。打开数据文件，从位置为 256 的那个地方开始顺序扫描直到找到 offset 为 368801 的那条 Message。 这套机制是建立在 offset 为有序的基础上，利用 segment + 有序 offset + 稀疏索引 + 二分查找 + 顺序查找等多种手段来高效的查找数据！至此，消费者就能拿到需要处理的数据进行处理了。那每个消费者又是怎么记录自己消费的位置呢？在早期的版本中，消费者将消费到的 offset 维护 zookeeper 中，consumer 每间隔一段时间上报一次，这里容易导致重复消费，且性能不好！在新的版本中消费者消费到的 offset 已经直接维护在 kafk 集群的__consumer_offsets 这个 topic 中！","tags":["KafKa"],"categories":["KafKa"]},{"title":"KafKa实战问题","path":"/KafKa/KafKa实战问题/","content":"消息积压2024/9/23 消费者 逻辑 发生NPE导致—》 消费者不断重试消费—-》发生消息积压—-〉存在有的消息一直得不到消费 修改 consume.exception.retry.count = 1 //表示失败后 重试消费1次 二、MQ“避坑” 问题分类 消息积压 相关描述 在使用MQ的过程中由于种种原因导致消息消费不及时造成消息积压 相关CASE 原因 1、消费流程卡死2、 消息消费耗时过长 3、消费组客户端启动失败 4、消费线程过少，消费能力不够。 5、客户端在消费失败后设置reture CONSUME_FAILURE，一旦不能恢复会导致一直重试 最佳实践 1、消费逻辑的业务处理尽量时间不要太长，如果存在长耗时逻辑尽量异步处理2、不要过多和外系统进行交互，避免其他服务问题导致消费能力下降3、消费线程要对异常进行分类处理，不要发生异常轻易终止或者关闭消费节点的注册4、消息积压后，通过shovel可以应急处理，防止消息丢失5、对与单Partition消息消费在不需要保证有序的情况下开启并行消费6、发现问题及时扩容Partition并扩容消费者机器（注意消费者组的数量 和 Partition数量的权衡）7、优化消费逻辑，能异步处理的尽量异步处理8、消费失败不要使用CONSUME_FAILURE（导致消费重试）,可以使用RECONSUME_LATER，最后做好备份逻辑 问题分类 消息丢失 相关描述 在MQ使用过程由于MQ系统故障或者使用不当造成消息丢失 相关CASE 原因 1、mafka partition leader选举策略问题造成消息丢失。2、 数据可靠性级别未设置为ack=-1。 3、在机器重启过程时，异步发送消息还没处理完客户端已经被销毁。 4、消息过大造成发送失败 消息发送失败没有及时关注发送结果。5、 集群机器大面积宕机 部分业务存在超时丢弃消息逻辑 最佳实践 1、业务消费未执行成功不要返回消费成功。 2、程序退出先关闭Consumer和Producer。 3、如果对消息丢失0容忍可设置客户端 ack=-1。 4、做好集群容灾处理，针对mafka尽量保证partition均匀分布在所有Broker中。 5、不要发送超过1M以上消息 问题分类 重复消费 相关描述 在使用MQ过程中我们经常会遇到消息被重复消费问题，如果不能准确处理会导致线上问题 相关CASE 原因 1、绝大部分消息中间件均不能保证消息只被消费一次。 2、生产者重复生产消息。 最佳实践 消息消费需要严格幂等控制，实现幂等的方式有很多，有依赖分布式锁将并行改成串行的、也有依赖数据库的事务的、也有依赖与数据库记录的状态之间流转的状态机的； 问题分类 消息发送失败 相关描述 在使用MQ过程中会遇到使用不规范或者系统故障导致消息发送失败 相关CASE 原因 1、客户端是使用不规范，重复创建实例，造成大量系统资源消耗。2、 系统异常未能及时监控流量，没有限流、切流预案。 3、未处理发送结果 最佳实践 1、按照规范创建客户端，可采用Spring bean配置创建，保证一个消费组或生产者只有一个实例对象。2、 关注发送结果。3、 构建有效的流量监控应急预案。","tags":["KafKa"],"categories":["KafKa"]},{"title":"数据库慢查询","path":"/MySQL/数据库慢查询/","content":"一、遇到的慢查询问题1、表的索引区分度小导致的满查询为什么会命中不同的索引？ 索引的选择是优化器的工作，mysql的优化器选择索引时会极大的依赖“统计信息”。但是本质原因是原来的索引区分度很差 统计信息又是什么时候执行的？统计信息更新分为手动更新与自动更新。 手动更新 ANALYZE table UPDATE HISTOGRAM ON WITH BUCKETS //更新某个表的某个列的直方图 ANALYZE table DROP HISTOGRAM ON col_name //删除某个表的直方图 ANALYZE table // 更新某个表中索引各个字段的cardinality 自动更新： 增删索引，加列， 清空表等涉及数据修改的DDL时； 后台线程发现表更新的记录超过表记录总数的1/10时。 有触发过统计数据的日志吗？这是mysql的系统自发的行为，只能查看到最后一次更新的记录，没有历史日志。 DBA执行了analyze table后数据被覆盖了，忘记对analyze table命令前最后一次统计信息进行查看。 这类满查询的注意事项（1）判断索引区分度的方法 select count(distinct column) /select count(*)-- 索引区分度要大于70% -- （2）设置降级策略、设置限流 （3）添加系统告警","tags":["MySQL","后端开发"],"categories":["MySQL"]},{"title":"Java8新特性：Stream流","path":"/Java基础/Java8新特性：Stream流/","content":"前言在 java 中，涉及到对数组、集合等集合类元素的操作时，通常我们使用的是循环的方式进行逐个遍历处理，或者使用 stream 流的方式进行处理。 什么是 Stream？Stream（流）是一个来自数据源的元素队列并支持聚合操作,流在管道中传输， 并且可以在管道的节点上进行处理， 比如筛选， 排序，聚合等。 Stream（流）的组成包含:元素、数据源、聚合操作、内部迭代、Pipelining等。 创建 Stream 流1）stream() StreamString stream = stringList.stream(); 2）parallelStream() StreamString stringStream = stringList.parallelStream(); Stream 流常用操作1）forEach stringList.forEach(System.out::println); 2）map stringList.stream().map(i-i.equals(juejin)); 3）filter stringList.stream().filter(i-i.equals(juejin)); 4）limit integerList.stream().limit(3); 5）skip integerList.stream().skip(5).limit(3); 6）distinct integerList.stream().distinct().collect(Collectors.toList()); 7）sorted integerList.stream().sorted(); 8）sorted(Comparator com) integerList.stream().sorted(Comparator.comparing(Integer::intValue)); 9）Collectors 收集器 恒等处理 Collectors 所谓恒等处理，指的就是Stream的元素在经过Collector函数处理前后完全不变，例如toList()操作，只是最终将结果从Stream中取出放入到List对象中，并没有对元素本身做任何的更改处理 list.stream().collect(Collectors.toList());list.stream().collect(Collectors.toSet());list.stream().collect(Collectors.toCollection()); 归约汇总 Collectors 对于归约汇总类的操作，Stream流中的元素逐个遍历，进入到Collector处理函数中，然后会与上一个元素的处理结果进行合并处理，并得到一个新的结果，以此类推，直到遍历完成后，输出最终的结果 counting 统计流中的元素个数 summingInt 计算流中指定int字段的累加总和。针对不同类型的数字类型，有不同的方法，比如summingDouble等 averagingInt 计算流中指定int字段的平均值。针对不同类型的数字类型，有不同的方法，比如averagingLong等 joining 将流中所有元素（或者元素的指定字段）字符串值进行拼接，可以指定拼接连接符，或者首尾拼接字符 maxBy 根据给定的比较器，选择出值最大的元素 minBy 根据给定的比较器，选择出值最小的元素 分组分区 Collectors 仅仅是做一个常规的数据分组操作时，可以仅传入一个分组函数即可 public void groupBySubCompany() // 按照子公司维度将员工分组 MapString, ListEmployee resultMap = getAllEmployees().stream() .collect(Collectors.groupingBy(Employee::getSubCompany)); System.out.println(resultMap); 如果不仅需要分组，还需要对分组后的数据进行处理的时候，则需要同时给定分组函数以及值收集器 public void groupAndCaculate() // 按照子公司分组，并统计每个子公司的员工数 MapString, Long resultMap = getAllEmployees().stream() .collect(Collectors.groupingBy(Employee::getSubCompany, Collectors.counting())); System.out.println(resultMap); 总结简而言之，Stream API 提供了一种高效且易于使用的处理数据的方式。让程序员写出高效率、干净、简洁的代码。","tags":["Java基础"],"categories":["Java基础"]}]